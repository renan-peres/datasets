{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68eb25f0",
   "metadata": {},
   "source": [
    "# Scrape Americas Stocks and Fundamentals -- Financial Modeling Prep (FMP) API\n",
    "\n",
    "End‑to‑end pipeline to build an Americas universe (tickers + profiles), fetch quotes and key metrics/ratios, and persist everything into DuckDB.\n",
    "\n",
    "- Data sources (FMP stable APIs):\n",
    "  - Exchanges: https://financialmodelingprep.com/stable/available-exchanges\n",
    "  - Stock list/profiles (paged bulk): https://financialmodelingprep.com/stable/profile-bulk?part=0\n",
    "  - EOD bulk (daily): https://financialmodelingprep.com/stable/eod-bulk?date=YYYY-MM-DD\n",
    "  - Key metrics: https://financialmodelingprep.com/stable/key-metrics?symbol=AAPL&period=quarter&limit=100\n",
    "  - Ratios: https://financialmodelingprep.com/stable/ratios?symbol=AAPL&period=quarter&limit=100\n",
    "  - Index list: https://financialmodelingprep.com/stable/index-list\n",
    "  - Index quotes (light): https://financialmodelingprep.com/stable/historical-price-eod/light\n",
    "\n",
    "- Inputs\n",
    "  - Optional environment variable FMP_API_KEY to raise rate limits: export FMP_API_KEY=your_key\n",
    "\n",
    "- Outputs (DuckDB: americas.db)\n",
    "  - exchanges, profiles, tickers, quotes, key_metrics, indices, index_quotes\n",
    "\n",
    "- Run order\n",
    "  1) Setup and exchanges filter  2) Profiles (paged) → tickers  3) Load profiles\n",
    "  4) EOD bulk quotes  5) Load quotes  6) Key metrics + ratios → load\n",
    "  7) Exchanges table  8) Indices + quotes → load\n",
    "\n",
    "Notes\n",
    "- “Americas” classification uses FMP exchange metadata (region/country) and symbol suffix mapping; U.S. tickers with no suffix are included.\n",
    "- Network calls use simple retry/backoff and thread pools to balance speed and rate limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71867fd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Optionally set FMP_API_KEY in your shell to improve quotas: export FMP_API_KEY=your_key.\n",
    "- All outputs are persisted into DuckDB (americas.db). No static files are read or written by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62035ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API response type: <class 'list'>\n",
      "Processing list with 71 items\n",
      "Created SUFFIX_TO_EXCHANGE mapping with 65 entries\n",
      "Identified 14 Americas exchanges: AMEX, BUE, BVC, CBOE, CNQ, MEX, NASDAQ, NEO, NYSE, OTC, SAO, SGO, TSX, TSXV\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\")\n",
    "params={\"apikey\":API_KEY} if API_KEY else {}\n",
    "\n",
    "# Fetch exchange data with better error handling\n",
    "try:\n",
    "    raw=requests.get(\"https://financialmodelingprep.com/stable/available-exchanges\",params=params,timeout=30).json()\n",
    "    print(f\"API response type: {type(raw)}\")\n",
    "    \n",
    "    # Handle both potential response formats\n",
    "    if isinstance(raw, dict):\n",
    "        # Extract list from dictionary if possible\n",
    "        list_found = False\n",
    "        for k, v in raw.items():\n",
    "            if isinstance(v, list) and len(v) > 0:\n",
    "                raw = v\n",
    "                list_found = True\n",
    "                print(f\"Found list in key '{k}' with {len(v)} items\")\n",
    "                break\n",
    "        if not list_found:\n",
    "            print(\"No list found in response dictionary, using empty list\")\n",
    "            raw = []\n",
    "    elif not isinstance(raw, list):\n",
    "        print(f\"Unexpected response type: {type(raw)}, using empty list\")\n",
    "        raw = []\n",
    "        \n",
    "    # Log response size for debugging\n",
    "    if isinstance(raw, list):\n",
    "        print(f\"Processing list with {len(raw)} items\")\n",
    "except Exception as e:\n",
    "    print(f\"API request failed: {e}\")\n",
    "    raw = []\n",
    "\n",
    "ex_data = raw\n",
    "SUFFIX_TO_EXCHANGE = {}\n",
    "if isinstance(ex_data, list) and len(ex_data) > 0:\n",
    "    SUFFIX_TO_EXCHANGE = { \n",
    "        (i.get(\"symbolSuffix\") or \"\").strip().upper(): (i.get(\"exchange\") or \"\").strip().upper() \n",
    "        for i in ex_data \n",
    "        if (i.get(\"symbolSuffix\") or \"\").strip() and (i.get(\"exchange\") or \"\").strip() \n",
    "    }\n",
    "    print(f\"Created SUFFIX_TO_EXCHANGE mapping with {len(SUFFIX_TO_EXCHANGE)} entries\")\n",
    "\n",
    "# Infer Americas exchanges from API (no file reads)\n",
    "CTRY={\"UNITED STATES\",\"CANADA\",\"BRAZIL\",\"MEXICO\",\"ARGENTINA\",\"CHILE\",\"PERU\",\"COLOMBIA\",\"VENEZUELA\",\"URUGUAY\",\"PARAGUAY\",\"BOLIVIA\",\"ECUADOR\",\"GUYANA\",\"SURINAME\",\"FRENCH GUIANA\",\"JAMAICA\",\"TRINIDAD AND TOBAGO\",\"TRINIDAD & TOBAGO\",\"BARBADOS\",\"BAHAMAS\",\"BERMUDA\",\"CAYMAN ISLANDS\",\"PANAMA\",\"COSTA RICA\",\"GUATEMALA\",\"HONDURAS\",\"EL SALVADOR\",\"NICARAGUA\",\"DOMINICAN REPUBLIC\",\"HAITI\",\"PUERTO RICO\",\"BELIZE\",\"CURACAO\",\"ARUBA\",\"SAINT LUCIA\",\"GRENADA\",\"ST. VINCENT AND THE GRENADINES\"}\n",
    "ISO2={\"US\",\"CA\",\"BR\",\"MX\",\"AR\",\"CL\",\"PE\",\"CO\",\"VE\",\"UY\",\"PY\",\"BO\",\"EC\",\"GY\",\"SR\",\"GF\",\"JM\",\"TT\",\"BB\",\"BS\",\"BM\",\"KY\",\"PA\",\"CR\",\"GT\",\"HN\",\"SV\",\"NI\",\"DO\",\"H\",\"PR\",\"BZ\",\"LC\",\"GD\",\"VC\",\"CW\",\"AW\"}\n",
    "\n",
    "def _amer_exchange(rec: dict)->bool:\n",
    "    for k in (\"region\",\"continent\"):\n",
    "        v=rec.get(k)\n",
    "        if isinstance(v,str) and \"AMERICA\" in v.upper(): return True\n",
    "    for k in (\"country\",\"countryName\",\"countryCode\",\"country_code\",\"country_iso2\"):\n",
    "        v=rec.get(k); vu=v.strip().upper() if isinstance(v,str) else \"\"\n",
    "        if vu and ((len(vu)<=3 and vu in ISO2) or vu in CTRY or \"UNITED STATES\" in vu or \"LATIN AMERICA\" in vu): return True\n",
    "    return False\n",
    "\n",
    "EXCHANGES_AMERICAS=set()\n",
    "for it in ex_data:\n",
    "    if _amer_exchange(it):\n",
    "        exch=(it.get(\"exchange\") or \"\").strip(); acr=(it.get(\"acronym\") or \"\").strip(); mic=(it.get(\"mic\") or \"\").strip()\n",
    "        if exch: EXCHANGES_AMERICAS.add(exch.upper())\n",
    "        elif acr or mic: EXCHANGES_AMERICAS.add((acr or mic).upper())\n",
    "\n",
    "# Fallback: infer from known U.S./Americas exchange acronyms in the exchange name map\n",
    "if not EXCHANGES_AMERICAS:\n",
    "    for _, exch in SUFFIX_TO_EXCHANGE.items():\n",
    "        eu=exch.upper()\n",
    "        if any(x in eu for x in (\"NAS\",\"NYS\",\"ARC\",\"BATS\",\"OTC\",\"TSX\",\"TSXV\",\"CSE\",\"BOV\",\"MEX\",\"XBOV\",\"XMEX\")):\n",
    "            EXCHANGES_AMERICAS.add(eu)\n",
    "\n",
    "print(f\"Identified {len(EXCHANGES_AMERICAS)} Americas exchanges: {', '.join(sorted(EXCHANGES_AMERICAS))}\")\n",
    "if not EXCHANGES_AMERICAS:\n",
    "    # Use hardcoded fallback instead of raising error to allow continuing execution\n",
    "    print(\"WARNING: Could not infer any Americas exchanges from API payload. Using hardcoded fallbacks.\")\n",
    "    EXCHANGES_AMERICAS = {\"NYSE\",\"NASDAQ\",\"AMEX\",\"ARCX\",\"NYS\",\"NAS\",\"ARC\",\"BATS\",\"OTC\",\"TSX\",\"TSXV\",\"CSE\"}\n",
    "    print(f\"Using fallback exchanges: {', '.join(sorted(EXCHANGES_AMERICAS))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b09791",
   "metadata": {},
   "source": [
    "## Profiles (paged) → Universe\n",
    "\n",
    "Pull paged company profiles and keep only Americas listings (via symbol suffix→exchange map). Limit the investable universe to marketCap ≥ 1B and persist to DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "039a481f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11806, 36)\n",
      "shape: (5, 36)\n",
      "┌──────────────┬──────────┬─────────────┬─────────────┬───┬─────────┬─────────────┬───────┬────────┐\n",
      "│ lastDividend ┆ currency ┆ address     ┆ website     ┆ … ┆ country ┆ isActivelyT ┆ isEtf ┆ volume │\n",
      "│ ---          ┆ ---      ┆ ---         ┆ ---         ┆   ┆ ---     ┆ rading      ┆ ---   ┆ ---    │\n",
      "│ f64          ┆ str      ┆ str         ┆ str         ┆   ┆ str     ┆ ---         ┆ str   ┆ i64    │\n",
      "│              ┆          ┆             ┆             ┆   ┆         ┆ str         ┆       ┆        │\n",
      "╞══════════════╪══════════╪═════════════╪═════════════╪═══╪═════════╪═════════════╪═══════╪════════╡\n",
      "│ 0.29096      ┆ USD      ┆ JPMorgan    ┆ null        ┆ … ┆ US      ┆ true        ┆ false ┆ 0      │\n",
      "│              ┆          ┆ Trust I     ┆             ┆   ┆         ┆             ┆       ┆        │\n",
      "│ 0.092        ┆ USD      ┆ One Beacon  ┆ www.btim.co ┆ … ┆ US      ┆ true        ┆ false ┆ 0      │\n",
      "│              ┆          ┆ Street      ┆ m           ┆   ┆         ┆             ┆       ┆        │\n",
      "│ 1.11         ┆ USD      ┆ null        ┆ null        ┆ … ┆ US      ┆ true        ┆ true  ┆ 15940  │\n",
      "│ 0.74238      ┆ USD      ┆ Hans-BOeckl ┆ https://www ┆ … ┆ DE      ┆ false       ┆ false ┆ 2      │\n",
      "│              ┆          ┆ er-Strasse  ┆ .leg-wohnen ┆   ┆         ┆             ┆       ┆        │\n",
      "│              ┆          ┆ 38          ┆ .de/unte…   ┆   ┆         ┆             ┆       ┆        │\n",
      "│ 1.0295       ┆ USD      ┆ 333 West    ┆ null        ┆ … ┆ US      ┆ true        ┆ false ┆ 0      │\n",
      "│              ┆          ┆ Wacker Dr.  ┆             ┆   ┆         ┆             ┆       ┆        │\n",
      "└──────────────┴──────────┴─────────────┴─────────────┴───┴─────────┴─────────────┴───────┴────────┘\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, requests, polars as pl\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\")\n",
    "\n",
    "def fetch_profiles_paged(api_key: str|None=None, start_part: int=0, max_parts: int|None=None, sleep_s: float=0.0, max_retries: int=3, verbose: bool=False)->pl.DataFrame:\n",
    "    key=api_key or API_KEY; params={\"apikey\":key} if key else {}\n",
    "    # ensure exchange context\n",
    "    global SUFFIX_TO_EXCHANGE, EXCHANGES_AMERICAS\n",
    "    if 'SUFFIX_TO_EXCHANGE' not in globals() or 'EXCHANGES_AMERICAS' not in globals():\n",
    "        try:\n",
    "            ex_data=requests.get(\"https://financialmodelingprep.com/stable/available-exchanges\",params=params,timeout=60).json()\n",
    "            ex_data = ex_data if isinstance(ex_data,list) else []\n",
    "        except Exception: ex_data=[]\n",
    "        SUFFIX_TO_EXCHANGE = 'SUFFIX_TO_EXCHANGE' in globals() and SUFFIX_TO_EXCHANGE or { (i.get(\"symbolSuffix\") or \"\").strip().upper(): (i.get(\"exchange\") or \"\").strip().upper() for i in ex_data if (i.get(\"symbolSuffix\") or \"\").strip() and (i.get(\"exchange\") or \"\").strip() }\n",
    "        # Infer amer exchanges from payload directly (no file reads)\n",
    "        CTRY={\"UNITED STATES\",\"CANADA\",\"BRAZIL\",\"MEXICO\",\"ARGENTINA\",\"CHILE\",\"PERU\",\"COLOMBIA\",\"VENEZUELA\",\"URUGUAY\",\"PARAGUAY\",\"BOLIVIA\",\"ECUADOR\",\"GUYANA\",\"SURINAME\",\"FRENCH GUIANA\",\"JAMAICA\",\"TRINIDAD AND TOBAGO\",\"TRINIDAD & TOBAGO\",\"BARBADOS\",\"BAHAMAS\",\"BERMUDA\",\"CAYMAN ISLANDS\",\"PANAMA\",\"COSTA RICA\",\"GUATEMALA\",\"HONDURAS\",\"EL SALVADOR\",\"NICARAGUA\",\"DOMINICAN REPUBLIC\",\"HAITI\",\"PUERTO RICO\",\"BELIZE\",\"CURACAO\",\"ARUBA\",\"SAINT LUCIA\",\"GRENADA\",\"ST. VINCENT AND THE GRENADINES\"}\n",
    "        ISO2={\"US\",\"CA\",\"BR\",\"MX\",\"AR\",\"CL\",\"PE\",\"CO\",\"VE\",\"UY\",\"PY\",\"BO\",\"EC\",\"GY\",\"SR\",\"GF\",\"JM\",\"TT\",\"BB\",\"BS\",\"BM\",\"KY\",\"PA\",\"CR\",\"GT\",\"HN\",\"SV\",\"NI\",\"DO\",\"H\",\"PR\",\"BZ\",\"LC\",\"GD\",\"VC\",\"CW\",\"AW\"}\n",
    "        def amer(r):\n",
    "            if any(isinstance(r.get(k), str) and \"AMERICA\" in r[k].upper() for k in (\"region\",\"continent\")): return True\n",
    "            for k in (\"country\",\"countryCode\",\"country_code\",\"country_iso2\",\"countryName\"):\n",
    "                v=r.get(k); vu=v.strip().upper() if isinstance(v,str) else \"\"\n",
    "                if vu and ((len(vu)<=3 and vu in ISO2) or vu in CTRY or \"UNITED STATES\" in vu or \"LATIN AMERICA\" in vu): return True\n",
    "            return False\n",
    "        EXCHANGES_AMERICAS=set()\n",
    "        for it in ex_data:\n",
    "            if amer(it):\n",
    "                exch=(it.get(\"exchange\") or \"\").strip(); acr=(it.get(\"acronym\") or \"\").strip(); mic=(it.get(\"mic\") or \"\").strip()\n",
    "                if exch: EXCHANGES_AMERICAS.add(exch.upper())\n",
    "                elif acr or mic: EXCHANGES_AMERICAS.add((acr or mic).upper())\n",
    "        if not EXCHANGES_AMERICAS:\n",
    "            for _, exch in SUFFIX_TO_EXCHANGE.items():\n",
    "                eu=str(exch).upper()\n",
    "                if any(x in eu for x in (\"NAS\",\"NYS\",\"ARC\",\"BATS\",\"OTC\",\"TSX\",\"TSXV\",\"CSE\",\"BOV\",\"MEX\",\"XBOV\",\"XMEX\")):\n",
    "                    EXCHANGES_AMERICAS.add(eu)\n",
    "    _is_amer=lambda s: isinstance(s,str) and (SUFFIX_TO_EXCHANGE.get((\".\"+s.rsplit(\".\",1)[-1]).upper()) in EXCHANGES_AMERICAS if \".\" in s else True)\n",
    "\n",
    "    def _parse(txt: str)->list[dict]:\n",
    "        s=txt.lstrip()\n",
    "        if s.startswith('['):\n",
    "            try: return json.loads(s)\n",
    "            except Exception: pass\n",
    "        norm=txt.replace('}{','}\\n{'); rec=[]\n",
    "        for ln in (l.strip() for l in norm.splitlines() if l.strip()):\n",
    "            i=ln.find('{'); ln=ln[i:] if i>=0 else ln\n",
    "            if ln.startswith('{'):\n",
    "                try: o=json.loads(ln); rec.append(o) if isinstance(o,dict) else None\n",
    "                except Exception: pass\n",
    "        if rec: return rec\n",
    "        if ',' in txt and '\\n' in txt:\n",
    "            try: return pl.read_csv(StringIO(txt), ignore_errors=True).to_dicts()\n",
    "            except Exception: pass\n",
    "            try:\n",
    "                import pandas as pd; return pl.from_pandas(pd.read_csv(StringIO(txt), engine=\"python\", dtype=str, on_bad_lines=\"skip\").fillna(\"\")).to_dicts()\n",
    "            except Exception: pass\n",
    "            try:\n",
    "                import csv; return [dict({k:(v or \"\") for k,v in r.items()}) for r in csv.DictReader(StringIO(txt))]\n",
    "            except Exception: return []\n",
    "        return []\n",
    "\n",
    "    def _to_pl(records: list[dict])->pl.DataFrame:\n",
    "        if not records: return pl.DataFrame()\n",
    "        keys=list({k for r in records for k in r.keys()}); norm=[{k:r.get(k,None) for k in keys} for r in records]\n",
    "        try: return pl.DataFrame(norm, schema_overrides={k:pl.Utf8 for k in keys}, strict=False, infer_schema_length=len(norm))\n",
    "        except Exception:\n",
    "            try:\n",
    "                import pandas as pd; return pl.from_pandas(pd.DataFrame(norm).astype(\"string\").fillna(\"\"))\n",
    "            except Exception: return pl.DataFrame()\n",
    "\n",
    "    sess=requests.Session(); url=\"https://financialmodelingprep.com/stable/profile-bulk\"; frames=[]; part=start_part\n",
    "    while True:\n",
    "        if max_parts is not None and part>=start_part+max_parts: break\n",
    "        q={\"part\":part,\"datatype\":\"json\",**params}; attempt=0\n",
    "        while True:\n",
    "            try:\n",
    "                r=sess.get(url,params=q,timeout=120); status=r.status_code\n",
    "                if verbose: print(f\"GET {r.url[:80]}... -> {status}\")\n",
    "                if status in (429,) or status>=500:\n",
    "                    if attempt<max_retries: time.sleep((sleep_s or 0.5)*(2**attempt)); attempt+=1; continue\n",
    "                if status==403: return pl.DataFrame()\n",
    "                r.raise_for_status(); data=_parse(r.text)\n",
    "            except Exception:\n",
    "                return pl.DataFrame() if not frames else pl.concat(frames, how=\"vertical_relaxed\").unique(subset=[\"symbol\"], keep=\"last\")\n",
    "            break\n",
    "        if not isinstance(data,list): break\n",
    "        if not data:\n",
    "            if r.text.strip(): part+=1; time.sleep(sleep_s) if sleep_s>0 else None; continue\n",
    "            break\n",
    "        df=_to_pl(data)\n",
    "        if df.is_empty(): part+=1; continue\n",
    "        if \"Symbol\" in df.columns and \"symbol\" not in df.columns: df=df.rename({\"Symbol\":\"symbol\"})\n",
    "        df=df.filter(pl.col(\"symbol\").map_elements(_is_amer, return_dtype=pl.Boolean)) if \"symbol\" in df.columns else pl.DataFrame()\n",
    "        if not df.is_empty():\n",
    "            casts=[]\n",
    "            for c in (\"price\",\"beta\",\"lastDiv\",\"lastDividend\",\"change\",\"changes\",\"changePercentage\",\"changesPercentage\"): casts.append(pl.col(c).cast(pl.Float64, strict=False)) if c in df.columns else None\n",
    "            for c in (\"marketCap\",\"mktCap\",\"volume\",\"volAvg\",\"avgVolume\",\"averageVolume\"): casts.append(pl.col(c).cast(pl.Int64, strict=False)) if c in df.columns else None\n",
    "            df=df.with_columns(casts) if casts else df; frames.append(df)\n",
    "        part+=1; time.sleep(sleep_s) if sleep_s>0 else None\n",
    "    return pl.DataFrame() if not frames else pl.concat(frames, how=\"vertical_relaxed\").unique(subset=[\"symbol\"], keep=\"last\")\n",
    "\n",
    "profiles_df=fetch_profiles_paged(api_key=os.getenv(\"FMP_API_KEY\"))\n",
    "profiles_df=profiles_df.filter(pl.col(\"marketCap\").cast(pl.Int64, strict=False) >= 1_000_000_000)\n",
    "stock_tickers = profiles_df.select(\"symbol\").unique().sort(\"symbol\")\n",
    "print(profiles_df.shape)\n",
    "print(profiles_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e1698",
   "metadata": {},
   "source": [
    "### Save tickers → DuckDB\n",
    "Persist unique investable symbols into americas.db.tickers for downstream joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd1492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, polars as pl\n",
    "# Incremental load for stock_tickers (unique by symbol)\n",
    "if 'stock_tickers' in globals() and isinstance(stock_tickers, pl.DataFrame) and not stock_tickers.is_empty():\n",
    "    con = duckdb.connect('../americas.db')\n",
    "    # Register incoming dataframe\n",
    "    con.register('tickers_view', stock_tickers.to_pandas())\n",
    "    # Create table if it does not exist (empty schema clone)\n",
    "    con.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS stock_tickers AS\n",
    "        SELECT * FROM tickers_view LIMIT 0\n",
    "    \"\"\")\n",
    "    # Insert only new symbols\n",
    "    con.sql(\"\"\"\n",
    "        INSERT INTO stock_tickers\n",
    "        SELECT t.*\n",
    "        FROM tickers_view t\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1 FROM stock_tickers x WHERE x.symbol = t.symbol\n",
    "        )\n",
    "    \"\"\")\n",
    "    # (Optional) collect count of new rows inserted in this run\n",
    "    new_count = con.sql(\"SELECT COUNT(*) AS c FROM tickers_view WHERE symbol NOT IN (SELECT symbol FROM stock_tickers)\").fetchone()[0] if False else None\n",
    "    con.close()\n",
    "else:\n",
    "    print(\"No stock_tickers to save; skipping DuckDB load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9716a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, polars as pl\n",
    "# Incremental load for stock_profiles (unique by symbol) with robust dynamic casting\n",
    "if 'profiles_df' in globals() and isinstance(profiles_df, pl.DataFrame) and not profiles_df.is_empty():\n",
    "    cap_col = 'marketCap' if 'marketCap' in profiles_df.columns else ('mktCap' if 'mktCap' in profiles_df.columns else None)\n",
    "    if cap_col:\n",
    "        # Filter investable universe\n",
    "        filtered = profiles_df.filter(pl.col(cap_col).cast(pl.Int64, strict=False) >= 1_000_000_000)\n",
    "        if not filtered.is_empty():\n",
    "            con = duckdb.connect('../americas.db')\n",
    "            table_exists = False\n",
    "            try:\n",
    "                table_exists = bool(con.execute(\"SELECT 1 FROM information_schema.tables WHERE table_name='stock_profiles'\").fetchone())\n",
    "            except Exception:\n",
    "                table_exists = False\n",
    "\n",
    "            if not table_exists:\n",
    "                # Detect boolean-like columns (string reps of true/false only)\n",
    "                bool_like = []\n",
    "                for c in filtered.columns:\n",
    "                    try:\n",
    "                        vals = filtered.select(pl.col(c).cast(pl.Utf8, strict=False).str.to_lowercase().drop_nulls().unique()).to_series().to_list()\n",
    "                        if vals and all(v in ('true','false') for v in vals):\n",
    "                            bool_like.append(c)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if bool_like:\n",
    "                    casts = [\n",
    "                        pl.when(pl.col(c).cast(pl.Utf8, strict=False).str.to_lowercase()==\"true\").then(pl.lit(1))\n",
    "                          .when(pl.col(c).cast(pl.Utf8, strict=False).str.to_lowercase()==\"false\").then(pl.lit(0))\n",
    "                          .otherwise(pl.lit(None)).alias(c)\n",
    "                        for c in bool_like\n",
    "                    ]\n",
    "                    filtered = filtered.with_columns(casts)\n",
    "                # Basic numeric casts for common fields\n",
    "                float_cols = [c for c in (\"price\",\"beta\",\"lastDiv\",\"lastDividend\",\"change\",\"changes\",\"changePercentage\",\"changesPercentage\") if c in filtered.columns]\n",
    "                int_cols = [c for c in (\"marketCap\",\"mktCap\",\"volume\",\"volAvg\",\"avgVolume\",\"averageVolume\") if c in filtered.columns]\n",
    "                casts = [pl.col(c).cast(pl.Float64, strict=False) for c in float_cols] + [pl.col(c).cast(pl.Int64, strict=False) for c in int_cols]\n",
    "                if casts:\n",
    "                    filtered = filtered.with_columns(casts)\n",
    "                # Create table schema clone + initial load\n",
    "                con.register('profiles_incoming_initial', filtered.to_pandas())\n",
    "                con.sql(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS stock_profiles AS\n",
    "                    SELECT * FROM profiles_incoming_initial LIMIT 0\n",
    "                \"\"\")\n",
    "                con.sql(\"INSERT INTO stock_profiles SELECT * FROM profiles_incoming_initial\")\n",
    "                con.close()\n",
    "            else:\n",
    "                # Existing table: build insert aligned to destination schema while avoiding lower() on numeric sources\n",
    "                con.register('profiles_incoming_raw', filtered.to_pandas())\n",
    "                schema_rows = con.execute(\"PRAGMA table_info('stock_profiles')\").fetchall()\n",
    "                # Map source column polars dtypes to simple strings\n",
    "                src_type_map = {c: str(t) for c, t in zip(filtered.columns, filtered.dtypes)}\n",
    "                numeric_prefixes = (\"Int\",\"UInt\",\"Float\",\"Decimal\")\n",
    "                select_exprs = []\n",
    "                dest_cols = []\n",
    "                for cid, name, dtype, *_ in schema_rows:\n",
    "                    dest_cols.append(name)\n",
    "                    upper_type = (dtype or '').upper()\n",
    "                    if name == 'symbol':\n",
    "                        expr = f\"p.{name} AS {name}\"\n",
    "                    elif name in filtered.columns:\n",
    "                        src_t = src_type_map.get(name, \"\")\n",
    "                        is_src_numeric = any(src_t.startswith(pref) for pref in numeric_prefixes)\n",
    "                        if 'INT' in upper_type:\n",
    "                            if is_src_numeric:\n",
    "                                # Source already numeric -> direct cast\n",
    "                                expr = f\"TRY_CAST(p.{name} AS {upper_type}) AS {name}\"\n",
    "                            else:\n",
    "                                # Source textual -> handle boolean-like strings then try numeric cast\n",
    "                                expr = (\n",
    "                                    f\"CASE WHEN lower(CAST(p.{name} AS VARCHAR))='true' THEN 1 \"\n",
    "                                    f\"WHEN lower(CAST(p.{name} AS VARCHAR))='false' THEN 0 \"\n",
    "                                    f\"ELSE TRY_CAST(p.{name} AS {upper_type}) END AS {name}\"\n",
    "                                )\n",
    "                        elif any(t in upper_type for t in ['DOUBLE','FLOAT','REAL','DECIMAL']):\n",
    "                            expr = f\"TRY_CAST(p.{name} AS {upper_type}) AS {name}\"\n",
    "                        else:\n",
    "                            # Leave as-is\n",
    "                            expr = f\"p.{name} AS {name}\"\n",
    "                    else:\n",
    "                        null_cast_type = upper_type if upper_type else 'VARCHAR'\n",
    "                        expr = f\"CAST(NULL AS {null_cast_type}) AS {name}\"\n",
    "                    select_exprs.append(expr)\n",
    "                insert_sql = f\"\"\"\n",
    "                    INSERT INTO stock_profiles ({','.join(dest_cols)})\n",
    "                    SELECT {','.join(select_exprs)}\n",
    "                    FROM profiles_incoming_raw p\n",
    "                    WHERE NOT EXISTS (\n",
    "                        SELECT 1 FROM stock_profiles x WHERE x.symbol = p.symbol\n",
    "                    )\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    con.sql(insert_sql)\n",
    "                except Exception as e:\n",
    "                    print(f\"Incremental insert failed: {e}\")\n",
    "                con.close()\n",
    "        else:\n",
    "            print(\"No stock_profiles with marketCap >= 1,000,000,000; skipping DuckDB load\")\n",
    "    else:\n",
    "        print(\"No market cap column found; skipping DuckDB load for stock_profiles\")\n",
    "else:\n",
    "    print(\"profiles_df is empty or undefined; skipping DuckDB load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f115d6",
   "metadata": {},
   "source": [
    "## EOD Bulk Quotes (2010‑01‑01 → today)\n",
    "\n",
    "Parallel fetch daily bulk EOD, filter to Americas + investable tickers, and stage for DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3eff3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel EOD Bulk (full or incremental range), filtered by `stock_tickers` + logging\n",
    "import os, requests, polars as pl, concurrent.futures as cf, logging, threading\n",
    "from io import StringIO\n",
    "from datetime import date as _date, timedelta\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "log = logging.getLogger(\"eod\")\n",
    "\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\"); params={\"apikey\":API_KEY} if API_KEY else {}\n",
    "SESSION = requests.Session()\n",
    "# Enlarge connection pool to avoid 'Connection pool is full' warnings under concurrency\n",
    "try:\n",
    "    ADAPTER = HTTPAdapter(pool_connections=128, pool_maxsize=128)\n",
    "    SESSION.mount('https://', ADAPTER); SESSION.mount('http://', ADAPTER)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Ensure filter context exists even if setup cells weren't run\n",
    "if 'SUFFIX_TO_EXCHANGE' not in globals(): SUFFIX_TO_EXCHANGE = {}\n",
    "if 'EXCHANGES_AMERICAS' not in globals(): EXCHANGES_AMERICAS = set()\n",
    "\n",
    "# business-day date range\n",
    "def date_range(start: str, end: str, weekdays_only: bool=True):\n",
    "    y,m,d=map(int,start.split(\"-\")); ye,me,de=map(int,end.split(\"-\")); dt=_date(y,m,d); end_dt=_date(ye,me,de)\n",
    "    while dt<=end_dt:\n",
    "        if not weekdays_only or dt.weekday()<5: yield dt.isoformat()\n",
    "        dt+=timedelta(days=1)\n",
    "\n",
    "_is_amer=lambda s: isinstance(s,str) and (SUFFIX_TO_EXCHANGE.get((\".\"+s.rsplit(\".\",1)[-1]).upper()) in EXCHANGES_AMERICAS if \".\" in s else True)\n",
    "\n",
    "# daily fetch with retry/backoff\n",
    "def fetch_one_day(ds:str, allowed_symbols: set[str]|None=None, max_retries:int=3)->pl.DataFrame:\n",
    "    attempt=0\n",
    "    while True:\n",
    "        try:\n",
    "            r=SESSION.get(f\"https://financialmodelingprep.com/stable/eod-bulk?date={ds}\", params=params, timeout=120); status=r.status_code\n",
    "            if status in (429,) or status>=500:\n",
    "                if attempt<max_retries:\n",
    "                    import time; log.warning(f\"{ds} -> {status}, retry {attempt+1}/{max_retries}\")\n",
    "                    time.sleep(0.5*(2**attempt)); attempt+=1; continue\n",
    "            r.raise_for_status()\n",
    "            # Read potentially mixed-type numeric columns as strings to avoid parse errors, then cast below\n",
    "            df=pl.read_csv(\n",
    "                StringIO(r.text),\n",
    "                try_parse_dates=False,\n",
    "                schema_overrides={\"open\": pl.Utf8, \"high\": pl.Utf8, \"low\": pl.Utf8, \"close\": pl.Utf8, \"adjClose\": pl.Utf8, \"volume\": pl.Utf8},\n",
    "                infer_schema_length=1000,\n",
    "            )\n",
    "            if df.is_empty(): log.debug(f\"{ds} -> 0 rows\"); return df\n",
    "            n0=len(df); df=df.filter(pl.col(\"symbol\").map_elements(_is_amer, return_dtype=pl.Boolean)); n1=len(df)\n",
    "            if df.is_empty(): log.debug(f\"{ds} -> amer 0/{n0}\"); return df\n",
    "            if allowed_symbols: df=df.filter(pl.col(\"symbol\").is_in(allowed_symbols)); n2=len(df)\n",
    "            else: n2=n1\n",
    "            log.debug(f\"{ds} -> raw={n0} amer={n1} allowed={n2}\")\n",
    "            return df.with_columns([\n",
    "                pl.col(\"date\").cast(pl.Utf8).str.to_date(\"%Y-%m-%d\"),\n",
    "                pl.col(\"open\").cast(pl.Float64, strict=False), pl.col(\"high\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"low\").cast(pl.Float64, strict=False), pl.col(\"close\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"adjClose\").cast(pl.Float64, strict=False),\n",
    "                # Cast volume via Float -> rounded Int to handle occasional fractional values\n",
    "                pl.col(\"volume\").cast(pl.Float64, strict=False).round(0).cast(pl.Int64, strict=False)\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            if attempt<max_retries:\n",
    "                import time; log.warning(f\"{ds} -> error {e!r}, retry {attempt+1}/{max_retries}\")\n",
    "                time.sleep(0.5*(2**attempt)); attempt+=1; continue\n",
    "            log.error(f\"{ds} -> failed after {max_retries} retries: {e!r}\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "# Parallel over full or supplied range, with optional streaming insert\n",
    "def parallel_fetch_eod_bulk_range(start_date:str, end_date:str,\n",
    "                                  max_workers:int|None=None,\n",
    "                                  weekdays_only:bool=True,\n",
    "                                  allowed_symbols: set[str] | None = None,\n",
    "                                  days: list[str] | None = None,\n",
    "                                  skip_existing: bool = True,\n",
    "                                  existing_dates: set[str] | None = None,\n",
    "                                  insert_into_duckdb: bool = False,\n",
    "                                  duckdb_path: str = '../americas.db') -> pl.DataFrame:\n",
    "    \"\"\"Fetch EOD bulk data.\n",
    "    Optimizations:\n",
    "      - pass precomputed days list\n",
    "      - skip days already in stock_quotes table (existing_dates)\n",
    "      - optionally stream each fetched day into DuckDB (idempotent insert on (date,symbol))\n",
    "    \"\"\"\n",
    "    all_days = days if days is not None else list(date_range(start_date, end_date, weekdays_only=weekdays_only))\n",
    "    if skip_existing and existing_dates:\n",
    "        fetch_days=[d for d in all_days if d not in existing_dates]\n",
    "    else:\n",
    "        fetch_days=all_days\n",
    "    if not fetch_days:\n",
    "        log.info(\"No new days to fetch (incremental). Returning empty DataFrame.\")\n",
    "        return pl.DataFrame()\n",
    "    if max_workers is None:\n",
    "        import os as _os; max_workers=min(32, max(8, (_os.cpu_count() or 8)*2))\n",
    "    log.info(f\"EOD bulk {fetch_days[0]}→{fetch_days[-1]}: {len(fetch_days)} new days (of {len(all_days)} total), workers={max_workers}, symbols={'all' if not allowed_symbols else len(allowed_symbols)}\")\n",
    "\n",
    "    con = None\n",
    "    lock = threading.Lock()\n",
    "    if insert_into_duckdb:\n",
    "        import duckdb\n",
    "        con = duckdb.connect(duckdb_path)\n",
    "        # Ensure table exists\n",
    "        con.execute(\"CREATE TABLE IF NOT EXISTS stock_quotes (date DATE, symbol VARCHAR, open DOUBLE, high DOUBLE, low DOUBLE, close DOUBLE, adjClose DOUBLE, volume BIGINT)\")\n",
    "        # Create composite index (DuckDB 1.0 lacks indexes; rely on NOT EXISTS checks later)\n",
    "\n",
    "    frames=[]; done=0; step=max(1, len(fetch_days)//20)\n",
    "    with cf.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs={ex.submit(fetch_one_day, ds, allowed_symbols): ds for ds in fetch_days}\n",
    "        for fut in cf.as_completed(futs):\n",
    "            ds=futs[fut]\n",
    "            f=fut.result(); done+=1\n",
    "            if isinstance(f, pl.DataFrame) and not f.is_empty():\n",
    "                if insert_into_duckdb and con is not None:\n",
    "                    # Insert only new (date,symbol)\n",
    "                    try:\n",
    "                        import duckdb\n",
    "                        with lock:\n",
    "                            con.register('__new_day', f.to_pandas())\n",
    "                            con.execute(\"\"\"\n",
    "                                INSERT INTO stock_quotes\n",
    "                                SELECT n.* FROM __new_day n\n",
    "                                WHERE NOT EXISTS (\n",
    "                                    SELECT 1 FROM stock_quotes q WHERE q.date = n.date AND q.symbol = n.symbol\n",
    "                                )\n",
    "                            \"\"\")\n",
    "                            con.unregister('__new_day')\n",
    "                    except Exception as e:\n",
    "                        log.warning(f\"DuckDB insert failed for {ds}: {e}\")\n",
    "                else:\n",
    "                    frames.append(f)\n",
    "            if done%step==0 or done==len(fetch_days): log.info(f\"Progress {done}/{len(fetch_days)} days (fetched {len(frames)} non-empty frames)\")\n",
    "    if con is not None:\n",
    "        con.close()\n",
    "    out = pl.DataFrame() if not frames else pl.concat(frames, how=\"vertical_relaxed\").unique(subset=[\"date\",\"symbol\"], keep=\"last\").sort([\"date\",\"symbol\"])\n",
    "    log.info(f\"Combined rows (not counting already inserted days): {0 if out.is_empty() else out.height}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575560f",
   "metadata": {},
   "source": [
    "### Extract Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d65c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: [('exchanges',), ('index_list',), ('index_quotes',), ('risk_premium',), ('stock_metrics',), ('stock_profiles',), ('stock_quotes',), ('stock_tickers',)]\n",
      "Fetching EOD bulk from 2025-09-17 to 2025-09-17\n"
     ]
    }
   ],
   "source": [
    "import duckdb as db\n",
    "from datetime import date  # or: from datetime import date as _date\n",
    "\n",
    "con = db.connect('../americas.db')\n",
    "\n",
    "# list tables\n",
    "tables = con.sql(\"SHOW TABLES\").fetchall()\n",
    "print(\"Tables in the database:\", tables)\n",
    "\n",
    "# get last quote date if table exists\n",
    "max_date = None\n",
    "if any(t[0] == 'stock_quotes' for t in tables):\n",
    "    max_date = con.sql(\"SELECT MAX(date) AS max_date FROM stock_quotes\").fetchone()[0]\n",
    "\n",
    "# Ensure string ISO format (DuckDB may return date/datetime object)\n",
    "if max_date:\n",
    "    # If you want to resume AFTER last stored day uncomment next line and import timedelta:\n",
    "    # from datetime import timedelta; max_date = (max_date + timedelta(days=1))\n",
    "    start_date = (max_date.date().isoformat() if hasattr(max_date, \"date\") else max_date.isoformat()) if hasattr(max_date, \"isoformat\") else str(max_date)\n",
    "else:\n",
    "    start_date = '2010-01-01'\n",
    "\n",
    "end_date = date.today().isoformat()\n",
    "print(f\"Fetching EOD bulk from {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c8e5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing quote days: 4099\n"
     ]
    }
   ],
   "source": [
    "# Gather existing quote dates to enable incremental skipping\n",
    "import duckdb as _db\n",
    "_existing_dates=set()\n",
    "try:\n",
    "    _con=_db.connect('../americas.db')\n",
    "    if _con.execute(\"SELECT 1 FROM information_schema.tables WHERE table_name='stock_quotes'\").fetchone():\n",
    "        _existing_dates={r[0].isoformat() if hasattr(r[0],'isoformat') else str(r[0]) for r in _con.execute(\"SELECT DISTINCT date FROM stock_quotes\").fetchall()}\n",
    "    _con.close()\n",
    "except Exception as e:\n",
    "    print(f\"Could not load existing quote dates: {e}\")\n",
    "print(f\"Existing quote days: {len(_existing_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2bd309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 11:47:45,942 INFO EOD bulk 2025-09-17→2025-09-17: 1 new days (of 1 total), workers=32, symbols=11806\n",
      "2025-09-17 11:48:51,793 INFO Progress 1/1 days (fetched 0 non-empty frames)\n",
      "2025-09-17 11:48:51,797 INFO Combined rows (not counting already inserted days): 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "No new in-memory rows (all inserted directly or nothing new).\n"
     ]
    }
   ],
   "source": [
    "# Incremental EOD fetch using optimized function with skipping + streaming inserts\n",
    "import polars as pl\n",
    "allowed_symbols = set(stock_tickers.get_column(\"symbol\").to_list()) if 'stock_tickers' in globals() else None\n",
    "\n",
    "# Only fetch new days and stream insert directly to DuckDB (reduces memory and time for long histories)\n",
    "parallel_month_df = parallel_fetch_eod_bulk_range(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    allowed_symbols=allowed_symbols,\n",
    "    skip_existing=True,\n",
    "    existing_dates=_existing_dates,\n",
    "    insert_into_duckdb=True,  # Stream directly\n",
    ")\n",
    "\n",
    "# For quick inspection show just last few rows newly fetched (if any collected in-memory)\n",
    "print(parallel_month_df.shape)\n",
    "if not parallel_month_df.is_empty():\n",
    "    print(\"Unique Symbols (new batch):\", parallel_month_df.select(pl.col(\"symbol\").n_unique()).item())\n",
    "    print(parallel_month_df.tail())\n",
    "else:\n",
    "    print(\"No new in-memory rows (all inserted directly or nothing new).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91476ca0",
   "metadata": {},
   "source": [
    "### Load quotes → DuckDB\n",
    "Create or replace americas.db.quotes from the staged EOD bulk dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd0d8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new stock_quotes data to load.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 9, 17, 0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb, polars as pl\n",
    "if 'parallel_month_df' in globals() and isinstance(parallel_month_df, pl.DataFrame) and not parallel_month_df.is_empty():\n",
    "    con = duckdb.connect('../americas.db')\n",
    "    con.register('new_quotes', parallel_month_df.to_pandas())\n",
    "\n",
    "    # Create table if missing\n",
    "    con.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS stock_quotes AS\n",
    "        SELECT * FROM new_quotes LIMIT 0\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert only rows whose (date,symbol) key not present\n",
    "    con.sql(\"\"\"\n",
    "        INSERT INTO stock_quotes\n",
    "        SELECT nq.*\n",
    "        FROM new_quotes nq\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1 FROM stock_quotes q\n",
    "            WHERE q.date = nq.date\n",
    "              AND q.symbol = nq.symbol\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    con.close()\n",
    "else:\n",
    "    print(\"No new stock_quotes data to load.\")\n",
    "\n",
    "con = db.connect('../americas.db')\n",
    "con.sql(\"SELECT MAX(date) AS max_date FROM stock_quotes\").fetchone()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf326c8c",
   "metadata": {},
   "source": [
    "## Metrics and Ratios (quarterly)\n",
    "\n",
    "Parallel fetch per‑symbol key metrics and ratios (last 100 periods), normalize/cast, join on (symbol, date, fiscalYear, period), and load to DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5450e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Metrics - total symbols to consider: 11806\n",
      "Existing metrics max(date): None\n",
      "Computed quarterly lookback threshold_date: None\n",
      "Latest API available date (probe): None\n",
      "Fetching metrics for 11806 symbols (threshold_date=None) with 16 workers...\n",
      "Progress: 500/11806 symbols, collected 425 non-empty frames, elapsed 0.5m\n",
      "Progress: 1000/11806 symbols, collected 845 non-empty frames, elapsed 1.3m\n",
      "Progress: 1500/11806 symbols, collected 1220 non-empty frames, elapsed 2.0m\n",
      "Progress: 2000/11806 symbols, collected 1621 non-empty frames, elapsed 3.5m\n",
      "Progress: 2500/11806 symbols, collected 2049 non-empty frames, elapsed 2.7m\n",
      "Progress: 3000/11806 symbols, collected 2410 non-empty frames, elapsed 3.0m\n",
      "Progress: 3500/11806 symbols, collected 2818 non-empty frames, elapsed 3.4m\n",
      "Progress: 4000/11806 symbols, collected 3033 non-empty frames, elapsed 3.6m\n",
      "Progress: 4500/11806 symbols, collected 3329 non-empty frames, elapsed 4.9m\n",
      "Progress: 5000/11806 symbols, collected 3674 non-empty frames, elapsed 4.6m\n",
      "Progress: 5500/11806 symbols, collected 4028 non-empty frames, elapsed 5.2m\n",
      "Progress: 6000/11806 symbols, collected 4254 non-empty frames, elapsed 5.8m\n",
      "Progress: 6500/11806 symbols, collected 4675 non-empty frames, elapsed 7.7m\n",
      "Progress: 7000/11806 symbols, collected 5066 non-empty frames, elapsed 7.4m\n",
      "Progress: 7500/11806 symbols, collected 5477 non-empty frames, elapsed 7.6m\n",
      "Progress: 8000/11806 symbols, collected 5862 non-empty frames, elapsed 7.8m\n",
      "Progress: 8500/11806 symbols, collected 6182 non-empty frames, elapsed 8.3m\n",
      "Progress: 9000/11806 symbols, collected 6534 non-empty frames, elapsed 8.5m\n",
      "Progress: 9500/11806 symbols, collected 6912 non-empty frames, elapsed 8.7m\n",
      "Progress: 10000/11806 symbols, collected 7289 non-empty frames, elapsed 10.3m\n",
      "Progress: 10500/11806 symbols, collected 7642 non-empty frames, elapsed 9.7m\n",
      "Progress: 11000/11806 symbols, collected 7961 non-empty frames, elapsed 10.3m\n",
      "Progress: 11500/11806 symbols, collected 8244 non-empty frames, elapsed 11.0m\n",
      "Progress: 11806/11806 symbols, collected 8445 non-empty frames, elapsed 11.5m\n",
      "merged_df rows (lookback+current): 543977\n",
      "shape: (5, 9)\n",
      "┌────────────┬────────────┬───────────┬────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
      "│ symbol     ┆ date       ┆ fiscalYea ┆ period ┆ … ┆ marketCap ┆ enterpris ┆ evToSales ┆ evToOpera │\n",
      "│ ---        ┆ ---        ┆ r         ┆ ---    ┆   ┆ ---       ┆ eValue    ┆ ---       ┆ tingCashF │\n",
      "│ str        ┆ date       ┆ ---       ┆ str    ┆   ┆ f64       ┆ ---       ┆ f64       ┆ low       │\n",
      "│            ┆            ┆ str       ┆        ┆   ┆           ┆ f64       ┆           ┆ ---       │\n",
      "│            ┆            ┆           ┆        ┆   ┆           ┆           ┆           ┆ f64       │\n",
      "╞════════════╪════════════╪═══════════╪════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
      "│ BC-PC      ┆ 2009-12-31 ┆ 2009      ┆ Q4     ┆ … ┆ 1.1236e9  ┆ 1.4479e9  ┆ 2.202745  ┆ -314.7530 │\n",
      "│            ┆            ┆           ┆        ┆   ┆           ┆           ┆           ┆ 43        │\n",
      "│ NSSC       ┆ 2006-03-31 ┆ 2006      ┆ Q3     ┆ … ┆ 2.1974e8  ┆ 2.2245e8  ┆ 13.020365 ┆ 495.44082 │\n",
      "│            ┆            ┆           ┆        ┆   ┆           ┆           ┆           ┆ 8         │\n",
      "│ UEPEN      ┆ 2019-09-30 ┆ 2019      ┆ Q3     ┆ … ┆ 1.9684e10 ┆ 2.9195e10 ┆ 17.598128 ┆ 37.002909 │\n",
      "│ PRU        ┆ 2009-03-31 ┆ 2009      ┆ Q1     ┆ … ┆ 8.0283e9  ┆ 2.1005e10 ┆ 2.46397   ┆ 8.282864  │\n",
      "│ PARAUCO.SN ┆ 2023-09-30 ┆ 2023      ┆ Q3     ┆ … ┆ 1.1572e12 ┆ 2.2106e12 ┆ 34.353435 ┆ 63.160261 │\n",
      "└────────────┴────────────┴───────────┴────────┴───┴───────────┴───────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Incremental fetch of Key Metrics + Ratios (quarterly) with date-based skipping\n",
    "# Added: quarterly lookback logic -> always refetch previous full quarter to capture restatements.\n",
    "# Example: today=2025-09-01, existing max=2025-07-05 (Q3). We set threshold to start of previous quarter (2025-04-01)\n",
    "# so we re-fetch Q2 + Q3 data (rows with date >= 2025-04-01). Older quarters are skipped.\n",
    "\n",
    "import os, time, requests, polars as pl, concurrent.futures as cf, duckdb\n",
    "from typing import Any, Dict, List\n",
    "from datetime import date as _date, datetime\n",
    "\n",
    "API_KEY = os.getenv(\"FMP_API_KEY\")\n",
    "_common_params = {\"apikey\": API_KEY} if API_KEY else {}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper: symbol universe\n",
    "# ------------------------------------------------------------------\n",
    "def load_symbol_universe()->list[str]:\n",
    "    if 'stock_tickers' in globals() and isinstance(stock_tickers, pl.DataFrame) and 'symbol' in stock_tickers.columns:\n",
    "        return [s for s in stock_tickers.get_column('symbol').to_list() if isinstance(s,str) and s]\n",
    "    try:\n",
    "        con = duckdb.connect('../americas.db')\n",
    "        rows = con.sql(\"SELECT symbol FROM stock_tickers\").fetchall(); con.close()\n",
    "        return [r[0] for r in rows if isinstance(r[0], str) and r[0]]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "symbols = load_symbol_universe()\n",
    "print(f\"Key Metrics - total symbols to consider: {len(symbols)}\")\n",
    "if not symbols:\n",
    "    print(\"No symbols available; aborting stock_metrics fetch.\")\n",
    "    merged_df = pl.DataFrame()\n",
    "else:\n",
    "    # ------------------------------------------------------------------\n",
    "    # Existing max date (if table exists)\n",
    "    # ------------------------------------------------------------------\n",
    "    existing_max_date = None; metrics_table_exists = False\n",
    "    try:\n",
    "        con = duckdb.connect('../americas.db')\n",
    "        metrics_table_exists = bool(con.execute(\"SELECT 1 FROM information_schema.tables WHERE table_name='stock_metrics'\").fetchone())\n",
    "        if metrics_table_exists:\n",
    "            existing_max_date = con.execute(\"SELECT max(date) FROM stock_metrics\").fetchone()[0]\n",
    "        con.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not inspect existing stock_metrics table: {e}\")\n",
    "    if existing_max_date:\n",
    "        existing_max_date = existing_max_date.isoformat() if hasattr(existing_max_date,'isoformat') else str(existing_max_date)\n",
    "    print(f\"Existing stock_metrics max(date): {existing_max_date}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Quarterly lookback threshold determination\n",
    "    # ------------------------------------------------------------------\n",
    "    def _quarter_start(d: _date) -> _date:\n",
    "        return _date(d.year, ((d.month-1)//3)*3 + 1, 1)\n",
    "    def _prev_quarter_start(d: _date) -> _date:\n",
    "        qs = _quarter_start(d); m = qs.month - 3; y = qs.year\n",
    "        if m <= 0: m += 12; y -= 1\n",
    "        return _date(y, m, 1)\n",
    "\n",
    "    threshold_date = None\n",
    "    if existing_max_date:\n",
    "        try:\n",
    "            emd = datetime.strptime(existing_max_date, \"%Y-%m-%d\").date()\n",
    "            prev_q_start = _prev_quarter_start(emd)\n",
    "            threshold_date = prev_q_start.isoformat()\n",
    "        except Exception:\n",
    "            threshold_date = existing_max_date  # fallback original\n",
    "    # If no existing data we do full load (threshold_date None)\n",
    "    print(f\"Computed quarterly lookback threshold_date: {threshold_date}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # HTTP helpers\n",
    "    # ------------------------------------------------------------------\n",
    "    SESSION = requests.Session()\n",
    "    from requests.adapters import HTTPAdapter\n",
    "    try: SESSION.mount('https://', HTTPAdapter(pool_connections=64, pool_maxsize=64))\n",
    "    except Exception: pass\n",
    "\n",
    "    def _fetch_json(url: str, params: dict, max_retries: int = 3, backoff: float = 0.5):\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                r = SESSION.get(url, params=params, timeout=60); s = r.status_code\n",
    "                if s in (429,) or s >= 500:\n",
    "                    if attempt < max_retries: time.sleep(backoff * (2 ** attempt)); attempt += 1; continue\n",
    "                if s == 403: return []\n",
    "                r.raise_for_status(); data = r.json()\n",
    "                if isinstance(data, dict):\n",
    "                    for v in data.values():\n",
    "                        if isinstance(v, list): return v\n",
    "                    return []\n",
    "                return data if isinstance(data, list) else []\n",
    "            except Exception:\n",
    "                if attempt < max_retries: time.sleep(backoff * (2 ** attempt)); attempt += 1; continue\n",
    "                return []\n",
    "\n",
    "    # Probe one symbol to detect latest API date\n",
    "    latest_api_date = None\n",
    "    if symbols:\n",
    "        probe_symbol = symbols[0]\n",
    "        probe_metrics = _fetch_json(\"https://financialmodelingprep.com/stable/key-stock_metrics\", {\"symbol\": probe_symbol, \"period\":\"quarter\", \"limit\":1, **_common_params})\n",
    "        if probe_metrics:\n",
    "            md = probe_metrics[0].get('date') or probe_metrics[0].get('reportedDate')\n",
    "            if md: latest_api_date = md.split('T')[0]\n",
    "        if latest_api_date is None:\n",
    "            probe_ratios = _fetch_json(\"https://financialmodelingprep.com/stable/ratios\", {\"symbol\": probe_symbol, \"period\":\"quarter\", \"limit\":1, **_common_params})\n",
    "            if probe_ratios:\n",
    "                rd = probe_ratios[0].get('date') or probe_ratios[0].get('reportedDate')\n",
    "                if rd: latest_api_date = rd.split('T')[0]\n",
    "    print(f\"Latest API available date (probe): {latest_api_date}\")\n",
    "\n",
    "    if metrics_table_exists and existing_max_date and latest_api_date and latest_api_date <= existing_max_date:\n",
    "        # Even if 'up to date', we still might want to re-fetch previous quarter if threshold_date earlier\n",
    "        if threshold_date and threshold_date < existing_max_date:\n",
    "            print(\"Data current, but performing quarterly lookback refresh.\")\n",
    "        else:\n",
    "            print(\"Metrics table already up to date; skipping fetch.\")\n",
    "            merged_df = pl.DataFrame()\n",
    "    if 'merged_df' not in globals():\n",
    "        # ------------------------------------------------------------------\n",
    "        # Per-symbol fetch & merge\n",
    "        # ------------------------------------------------------------------\n",
    "        def fetch_symbol(symbol: str) -> pl.DataFrame:\n",
    "            km = _fetch_json(\"https://financialmodelingprep.com/stable/key-stock_metrics\", {\"symbol\": symbol, \"period\":\"quarter\", \"limit\":100, **_common_params})\n",
    "            rt = _fetch_json(\"https://financialmodelingprep.com/stable/ratios\", {\"symbol\": symbol, \"period\":\"quarter\", \"limit\":100, **_common_params})\n",
    "            if not km and not rt: return pl.DataFrame()\n",
    "            def to_df(records: List[Dict[str, Any]]) -> pl.DataFrame:\n",
    "                if not records: return pl.DataFrame()\n",
    "                for r in records:\n",
    "                    r['symbol'] = r.get('symbol', symbol)\n",
    "                    d = r.get('date') or r.get('reportedDate')\n",
    "                    if d: r['date'] = d.split('T')[0]\n",
    "                try: return pl.DataFrame(records)\n",
    "                except Exception:\n",
    "                    import pandas as pd; return pl.from_pandas(pd.DataFrame(records))\n",
    "            km_df = to_df(km); rt_df = to_df(rt)\n",
    "            if threshold_date and 'date' in km_df.columns and not km_df.is_empty(): km_df = km_df.filter(pl.col('date') >= threshold_date)\n",
    "            if threshold_date and 'date' in rt_df.columns and not rt_df.is_empty(): rt_df = rt_df.filter(pl.col('date') >= threshold_date)\n",
    "            if km_df.is_empty() and rt_df.is_empty(): return pl.DataFrame()\n",
    "            if km_df.is_empty(): out = rt_df\n",
    "            elif rt_df.is_empty(): out = km_df\n",
    "            else:\n",
    "                join_keys = [k for k in ('symbol','date','fiscalYear','period') if k in km_df.columns and k in rt_df.columns] or [k for k in ('symbol','date') if k in km_df.columns and k in rt_df.columns]\n",
    "                rt_only = [c for c in rt_df.columns if c not in km_df.columns]\n",
    "                if join_keys:\n",
    "                    out = km_df.join(rt_df.select(join_keys + rt_only), on=join_keys, how='full')\n",
    "                else:\n",
    "                    out = pl.concat([km_df, rt_df], how='vertical_relaxed')\n",
    "            if 'date' in out.columns:\n",
    "                out = out.with_columns(pl.col('date').cast(pl.Utf8).str.strptime(pl.Date, \"%Y-%m-%d\", strict=False))\n",
    "            return out\n",
    "\n",
    "        MAX_WORKERS = min(48, max(8, (os.cpu_count() or 8)*2)) if metrics_table_exists else min(32, max(8, (os.cpu_count() or 8)))\n",
    "        start_ts = time.time(); frames: List[pl.DataFrame] = []; submitted = 0; last_print = 0\n",
    "        print(f\"Fetching stock_metrics for {len(symbols)} symbols (threshold_date={threshold_date}) with {MAX_WORKERS} workers...\")\n",
    "        with cf.ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futures = {ex.submit(fetch_symbol, s): s for s in symbols}\n",
    "            for fut in cf.as_completed(futures):\n",
    "                sym = futures[fut]; submitted += 1\n",
    "                try:\n",
    "                    df = fut.result()\n",
    "                    if isinstance(df, pl.DataFrame) and not df.is_empty(): frames.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"{sym} failed: {e}\")\n",
    "                if (submitted - last_print) >= 500 or submitted == len(symbols):\n",
    "                    elapsed = (time.time() - start_ts)/60\n",
    "                    print(f\"Progress: {submitted}/{len(symbols)} symbols, collected {len(frames)} non-empty frames, elapsed {elapsed:.1f}m\")\n",
    "                    last_print = submitted\n",
    "        if not frames:\n",
    "            print(\"No stock_metrics/ratios rows fetched for lookback window.\")\n",
    "            merged_df = pl.DataFrame()\n",
    "        else:\n",
    "            # Schema alignment\n",
    "            union_cols = []\n",
    "            seen = set()\n",
    "            for f in frames:\n",
    "                for c in f.columns:\n",
    "                    if c not in seen:\n",
    "                        seen.add(c); union_cols.append(c)\n",
    "            key_order = [c for c in ('symbol','date','fiscalYear','period') if c in union_cols]\n",
    "            other_cols = [c for c in union_cols if c not in key_order]\n",
    "            ordered_cols = key_order + other_cols\n",
    "            aligned = []\n",
    "            \n",
    "            # First pass: determine common data types for numeric columns\n",
    "            numeric_cols = {}\n",
    "            for f in frames:\n",
    "                for col_name, dtype in zip(f.columns, f.dtypes):\n",
    "                    dtype_str = str(dtype)\n",
    "                    if any(dtype_str.startswith(prefix) for prefix in ['Int', 'Float']):\n",
    "                        if col_name not in numeric_cols:\n",
    "                            numeric_cols[col_name] = dtype\n",
    "                        elif str(numeric_cols[col_name]).startswith('Int') and dtype_str.startswith('Float'):\n",
    "                            # Promote to Float64 if we see both Int and Float\n",
    "                            numeric_cols[col_name] = pl.Float64\n",
    "            \n",
    "            for f in frames:\n",
    "                # Add missing columns\n",
    "                missing = [c for c in ordered_cols if c not in f.columns]\n",
    "                if missing:\n",
    "                    f = f.with_columns([pl.lit(None).alias(c) for c in missing])\n",
    "                \n",
    "                # Fix data types for numeric columns to ensure consistency\n",
    "                casts = []\n",
    "                for col_name, dtype in numeric_cols.items():\n",
    "                    if col_name in f.columns:\n",
    "                        # Ensure all numeric columns use the same data type (prefer Float64 for consistency)\n",
    "                        if dtype == pl.Float64 or str(dtype).startswith('Float'):\n",
    "                            casts.append(pl.col(col_name).cast(pl.Float64, strict=False))\n",
    "                \n",
    "                if casts:\n",
    "                    f = f.with_columns(casts)\n",
    "                \n",
    "                f = f.select(ordered_cols)\n",
    "                aligned.append(f)\n",
    "                \n",
    "            merged_df = pl.concat(aligned, how='vertical')\n",
    "            \n",
    "            # Filter by threshold date if provided\n",
    "            if threshold_date and 'date' in merged_df.columns:\n",
    "                # Clean up threshold date and convert to proper date\n",
    "                threshold_date_clean = threshold_date.split('T')[0] if 'T' in threshold_date else threshold_date\n",
    "                \n",
    "                # Use datetime to parse the date string and convert to a Date object\n",
    "                from datetime import datetime\n",
    "                threshold_date_obj = datetime.strptime(threshold_date_clean, \"%Y-%m-%d\").date()\n",
    "                \n",
    "                # Convert to polars Date expression\n",
    "                threshold_date_pl = pl.lit(threshold_date_obj).cast(pl.Date)\n",
    "                \n",
    "                # Filter with properly typed date\n",
    "                merged_df = merged_df.filter(pl.col('date') >= threshold_date_pl)\n",
    "                \n",
    "            base_cols = [c for c in ('symbol','date','fiscalYear','period') if c in merged_df.columns]\n",
    "            if base_cols:\n",
    "                merged_df = merged_df.unique(subset=base_cols, keep='last')\n",
    "            print(f\"merged_df rows (lookback+current): {merged_df.height}\")\n",
    "            try:\n",
    "                sample_cols = [c for c in ['symbol','date','fiscalYear','period'] if c in merged_df.columns]\n",
    "                sample_cols += [c for c in merged_df.columns if c not in sample_cols][:5]\n",
    "                print(merged_df.select(sample_cols).head())\n",
    "            except Exception:\n",
    "                print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b526d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7272587f21c4489c933ec67ac0591c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load to DuckDB (incremental append on (symbol,date,fiscalYear,period))\n",
    "import polars as pl\n",
    "if 'merged_df' in globals() and isinstance(merged_df, pl.DataFrame) and not merged_df.is_empty():\n",
    "    import duckdb\n",
    "    con = duckdb.connect('../americas.db')\n",
    "    # Ensure base types for key columns\n",
    "    casts = []\n",
    "    if 'symbol' in merged_df.columns: casts.append(pl.col('symbol').cast(pl.Utf8, strict=False))\n",
    "    if 'date' in merged_df.columns: casts.append(pl.col('date').cast(pl.Date, strict=False))\n",
    "    if 'fiscalYear' in merged_df.columns: casts.append(pl.col('fiscalYear').cast(pl.Int64, strict=False))\n",
    "    if 'period' in merged_df.columns: casts.append(pl.col('period').cast(pl.Utf8, strict=False))\n",
    "    merged_df = merged_df.with_columns(casts) if casts else merged_df\n",
    "\n",
    "    con.register('metrics_new', merged_df.to_pandas())\n",
    "    # Create table if not exists\n",
    "    con.execute(\"CREATE TABLE IF NOT EXISTS stock_metrics AS SELECT * FROM metrics_new LIMIT 0\")\n",
    "\n",
    "    # Determine natural key columns present\n",
    "    key_cols = [c for c in ('symbol','date','fiscalYear','period') if c in merged_df.columns]\n",
    "    if not key_cols:  # fallback\n",
    "        key_cols = [c for c in ('symbol','date') if c in merged_df.columns]\n",
    "\n",
    "    # Build NOT EXISTS predicate\n",
    "    predicate = ' AND '.join([f\"m.{c} = n.{c}\" for c in key_cols]) if key_cols else '1=0'\n",
    "\n",
    "    insert_sql = f\"\"\"\n",
    "        INSERT INTO stock_metrics\n",
    "        SELECT n.* FROM metrics_new n\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1 FROM stock_metrics m WHERE {predicate}\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        con.execute(insert_sql)\n",
    "        new_rows = con.execute(\"SELECT COUNT(*) FROM metrics_new n WHERE NOT EXISTS (SELECT 1 FROM stock_metrics m WHERE \" + predicate + \")\").fetchone()[0] if False else None\n",
    "    except Exception as e:\n",
    "        print(f\"Incremental stock_metrics insert failed: {e}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "else:\n",
    "    print(\"No new key stock_metrics/ratios data to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8d973",
   "metadata": {},
   "source": [
    "## Exchanges → DuckDB\n",
    "\n",
    "Fetch all exchanges, keep only Americas via heuristic, and persist to americas.db.exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe17d8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 6)\n",
      "shape: (5, 6)\n",
      "┌──────────┬───────────────────────────┬──────────────────┬─────────────┬──────────────┬───────────┐\n",
      "│ exchange ┆ name                      ┆ country          ┆ countryCode ┆ symbolSuffix ┆ delay     │\n",
      "│ ---      ┆ ---                       ┆ ---              ┆ ---         ┆ ---          ┆ ---       │\n",
      "│ str      ┆ str                       ┆ str              ┆ str         ┆ str          ┆ str       │\n",
      "╞══════════╪═══════════════════════════╪══════════════════╪═════════════╪══════════════╪═══════════╡\n",
      "│ AMEX     ┆ New York Stock Exchange   ┆ United States of ┆ US          ┆ N/A          ┆ Real-time │\n",
      "│          ┆ Arca                      ┆ America          ┆             ┆              ┆           │\n",
      "│ BUE      ┆ Buenos Aires Stock        ┆ Argentina        ┆ AR          ┆ .BA          ┆ 20 min    │\n",
      "│          ┆ Exchange                  ┆                  ┆             ┆              ┆           │\n",
      "│ BVC      ┆ Colombia Stock Exchange   ┆ Colombia         ┆ CO          ┆ .CL          ┆ 15 min    │\n",
      "│ CBOE     ┆ Chicago Board Options     ┆ United States of ┆ US          ┆ N/A          ┆ Real-time │\n",
      "│          ┆ Exchange                  ┆ America          ┆             ┆              ┆           │\n",
      "│ CNQ      ┆ Canadian Securities       ┆ Canada           ┆ CA          ┆ .CN          ┆ Real-time │\n",
      "│          ┆ Exchange                  ┆                  ┆             ┆              ┆           │\n",
      "└──────────┴───────────────────────────┴──────────────────┴─────────────┴──────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Exchanges → DuckDB (incremental)\n",
    "import os, requests, polars as pl\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\"); _params={\"apikey\":API_KEY} if API_KEY else {}\n",
    "\n",
    "try:\n",
    "    r=requests.get(\"https://financialmodelingprep.com/stable/available-exchanges\", params=_params, timeout=60); r.raise_for_status(); ex_data=r.json(); ex_data=ex_data if isinstance(ex_data,list) else []\n",
    "except Exception: ex_data=[]\n",
    "\n",
    "CTRY={\"UNITED STATES\",\"CANADA\",\"BRAZIL\",\"MEXICO\",\"ARGENTINA\",\"CHILE\",\"PERU\",\"COLOMBIA\",\"VENEZUELA\",\"URUGUAY\",\"PARAGUAY\",\"BOLIVIA\",\"ECUADOR\",\"GUYANA\",\"SURINAME\",\"FRENCH GUIANA\",\"JAMAICA\",\"TRINIDAD AND TOBAGO\",\"TRINIDAD & TOBAGO\",\"BARBADOS\",\"BAHAMAS\",\"BERMUDA\",\"CAYMAN ISLANDS\",\"PANAMA\",\"COSTA RICA\",\"GUATEMALA\",\"HONDURAS\",\"EL SALVADOR\",\"NICARAGUA\",\"DOMINICAN REPUBLIC\",\"HAITI\",\"PUERTO RICO\",\"BELIZE\",\"CURACAO\",\"ARUBA\",\"SAINT LUCIA\",\"GRENADA\",\"ST. VINCENT AND THE GRENADINES\"}\n",
    "ISO2={\"US\",\"CA\",\"BR\",\"MX\",\"AR\",\"CL\",\"PE\",\"CO\",\"VE\",\"UY\",\"PY\",\"BO\",\"EC\",\"GY\",\"SR\",\"GF\",\"JM\",\"TT\",\"BB\",\"BS\",\"BM\",\"KY\",\"PA\",\"CR\",\"GT\",\"HN\",\"SV\",\"NI\",\"DO\",\"H\",\"PR\",\"BZ\",\"LC\",\"GD\",\"VC\",\"CW\",\"AW\"}\n",
    "\n",
    "def is_amer_exchange(rec: dict)->bool:\n",
    "    for k in (\"region\",\"continent\"):\n",
    "        v=rec.get(k)\n",
    "        if isinstance(v,str) and \"AMERICA\" in v.upper(): return True\n",
    "    for k in (\"countryName\",\"country\",\"countryCode\",\"country_code\",\"country_iso2\"):\n",
    "        v=rec.get(k); vu=v.strip().upper() if isinstance(v,str) else \"\"\n",
    "        if vu and ((len(vu)<=3 and vu in ISO2) or vu in CTRY or \"UNITED STATES\" in vu or \"LATIN AMERICA\" in vu): return True\n",
    "    return False\n",
    "\n",
    "ex_df = pl.DataFrame([rec for rec in ex_data if isinstance(rec,dict) and is_amer_exchange(rec)])\n",
    "if not ex_df.is_empty():\n",
    "    ren={}; ren[\"countryName\"]=\"country\" if \"countryName\" in ex_df.columns and \"country\" not in ex_df.columns else None; ren={k:v for k,v in ren.items() if v}\n",
    "    ex_df = ex_df.rename(ren) if ren else ex_df\n",
    "    ex_df = ex_df.with_columns([pl.all().cast(pl.Utf8, strict=False)])\n",
    "\n",
    "print(ex_df.shape); print(ex_df.head())\n",
    "\n",
    "if not ex_df.is_empty():\n",
    "    import duckdb\n",
    "    con=duckdb.connect('../americas.db')\n",
    "    con.register('exchanges_new', ex_df.to_pandas())\n",
    "    con.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS exchanges AS\n",
    "        SELECT * FROM exchanges_new LIMIT 0\n",
    "    \"\"\")\n",
    "    \n",
    "    # Check which columns exist in the dataframe\n",
    "    columns = ex_df.columns\n",
    "    \n",
    "    # Use exchange as the primary key since it's always present\n",
    "    if 'exchange' in columns:\n",
    "        con.sql(\"\"\"\n",
    "            INSERT INTO exchanges\n",
    "            SELECT e.* FROM exchanges_new e\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM exchanges x\n",
    "                WHERE x.exchange = e.exchange\n",
    "            )\n",
    "        \"\"\")\n",
    "    con.close()\n",
    "else:\n",
    "    print(\"No Americas exchanges found from API; skipping DuckDB load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879a578",
   "metadata": {},
   "source": [
    "## Indices → DuckDB\n",
    "\n",
    "Fetch index list, keep those on Americas exchanges, then persist as americas.db.indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2c23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 4)\n",
      "shape: (5, 4)\n",
      "┌────────┬─────────────────────────────────┬──────────┬──────────┐\n",
      "│ symbol ┆ name                            ┆ exchange ┆ currency │\n",
      "│ ---    ┆ ---                             ┆ ---      ┆ ---      │\n",
      "│ str    ┆ str                             ┆ str      ┆ str      │\n",
      "╞════════╪═════════════════════════════════╪══════════╪══════════╡\n",
      "│ ^TTIN  ┆ S&P/TSX Capped Industrials Ind… ┆ TSX      ┆ CAD      │\n",
      "│ ^NYA   ┆ NYSE Composite                  ┆ NYSE     ┆ USD      │\n",
      "│ ^XAX   ┆ NYSE American Composite Index   ┆ NYSE     ┆ USD      │\n",
      "│ ^NYITR ┆ NYSE International 100 Index    ┆ NYSE     ┆ USD      │\n",
      "│ ^DJU   ┆ Dow Jones Utility Average       ┆ NASDAQ   ┆ USD      │\n",
      "└────────┴─────────────────────────────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Indices → DuckDB (incremental)\n",
    "import os, requests, polars as pl\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\"); _params={\"apikey\":API_KEY} if API_KEY else {}\n",
    "\n",
    "try:\n",
    "    r=requests.get(\"https://financialmodelingprep.com/stable/index-list\", params=_params, timeout=60); r.raise_for_status(); idx_data=r.json(); idx_data=idx_data if isinstance(idx_data,list) else []\n",
    "except Exception: idx_data=[]\n",
    "\n",
    "amer_exchanges = {e.upper() for e in EXCHANGES_AMERICAS} if 'EXCHANGES_AMERICAS' in globals() and EXCHANGES_AMERICAS else {\"NYSE\",\"NASDAQ\",\"AMEX\",\"ARCX\",\"NYS\",\"NAS\",\"ARC\",\"BATS\",\"OTC\",\"TSX\",\"TSXV\",\"CSE\",\"XTSE\",\"XTSX\",\"CHIC\",\"B3\",\"BMFBOVESPA\",\"BOV\",\"XBOV\",\"XMEX\",\"BMV\"}\n",
    "idx_df = pl.DataFrame([rec for rec in idx_data if isinstance(rec,dict) and str(rec.get(\"exchange\",\"\" )).upper() in amer_exchanges])\n",
    "if not idx_df.is_empty(): idx_df=idx_df.with_columns([pl.all().cast(pl.Utf8, strict=False)])\n",
    "\n",
    "print(idx_df.shape); print(idx_df.head())\n",
    "\n",
    "if not idx_df.is_empty():\n",
    "    import duckdb\n",
    "    con=duckdb.connect('../americas.db')\n",
    "    con.register('indices_new', idx_df.to_pandas())\n",
    "    con.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS index_list AS\n",
    "        SELECT * FROM indices_new LIMIT 0\n",
    "    \"\"\")\n",
    "    # Natural key = symbol (assumed unique for index_list list)\n",
    "    con.sql(\"\"\"\n",
    "        INSERT INTO index_list\n",
    "        SELECT i.* FROM indices_new i\n",
    "        WHERE NOT EXISTS (SELECT 1 FROM index_list x WHERE x.symbol = i.symbol)\n",
    "    \"\"\")\n",
    "    con.close()\n",
    "else:\n",
    "    print(\"No Americas index_list found from API; skipping DuckDB load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77159c2",
   "metadata": {},
   "source": [
    "## Index Quotes (historical light) → DuckDB\n",
    "\n",
    "Parallel fetch daily light quotes for Americas indices since 2010‑01‑01 and write to americas.db.index_quotes (unique by symbol,date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index symbols to fetch: 36\n",
      "(111558, 4)\n",
      "shape: (5, 4)\n",
      "┌─────────┬────────────┬────────────┬───────────┐\n",
      "│ symbol  ┆ date       ┆ price      ┆ volume    │\n",
      "│ ---     ┆ ---        ┆ ---        ┆ ---       │\n",
      "│ str     ┆ date       ┆ f64        ┆ i64       │\n",
      "╞═════════╪════════════╪════════════╪═══════════╡\n",
      "│ TX60.TS ┆ 2020-11-03 ┆ 948.200012 ┆ 94504821  │\n",
      "│ TX60.TS ┆ 2020-11-04 ┆ 952.299988 ┆ 179295563 │\n",
      "│ TX60.TS ┆ 2020-11-05 ┆ 968.52002  ┆ 127428692 │\n",
      "│ TX60.TS ┆ 2020-11-06 ┆ 966.869995 ┆ 118405024 │\n",
      "│ TX60.TS ┆ 2020-11-09 ┆ 980.429993 ┆ 246182781 │\n",
      "└─────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Index stock_quotes (light) → DuckDB\n",
    "import os, time, requests, polars as pl, concurrent.futures as cf\n",
    "from io import StringIO\n",
    "\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\"); _params={\"apikey\": API_KEY} if API_KEY else {}\n",
    "BASE_URL=\"https://financialmodelingprep.com/stable/historical-price-eod/light\"\n",
    "\n",
    "# 1) Gather index symbols\n",
    "index_symbols = sorted({s for s in idx_df.get_column('symbol').to_list() if isinstance(s,str) and s.strip()}) if 'idx_df' in globals() and isinstance(idx_df, pl.DataFrame) and not idx_df.is_empty() else []\n",
    "if not index_symbols:\n",
    "    try:\n",
    "        import duckdb; con=duckdb.connect('../americas.db'); res=con.sql(\"SELECT symbol FROM index_list\").fetchall(); con.close(); index_symbols=sorted({r[0] for r in res if isinstance(r[0],str) and r[0].strip()})\n",
    "    except Exception: index_symbols=[]\n",
    "print(f\"Index symbols to fetch: {len(index_symbols)}\")\n",
    "\n",
    "# 2) Fetch per symbol\n",
    "def fetch_index_quotes(symbol: str, start: str=\"2010-01-01\", max_retries: int=3, sleep_base: float=0.4)->pl.DataFrame:\n",
    "    q={\"symbol\":symbol,\"from\":start, **_params}; a=0\n",
    "    while True:\n",
    "        try:\n",
    "            r=requests.get(BASE_URL, params=q, timeout=90); s=r.status_code\n",
    "            if s in (429,) or s>=500:\n",
    "                if a<max_retries: time.sleep(sleep_base*(2**a)); a+=1; continue\n",
    "            if s==403: return pl.DataFrame()\n",
    "            r.raise_for_status(); data=r.json(); data=[data] if isinstance(data,dict) else data\n",
    "            if not isinstance(data,list) or not data: return pl.DataFrame()\n",
    "            df=pl.DataFrame(data)\n",
    "            casts=[]\n",
    "            if \"date\" in df.columns: casts.append(pl.col(\"date\").cast(pl.Utf8).str.to_date(\"%Y-%m-%d\"))\n",
    "            if \"price\" in df.columns: casts.append(pl.col(\"price\").cast(pl.Float64, strict=False))\n",
    "            if \"volume\" in df.columns: casts.append(pl.col(\"volume\").cast(pl.Int64, strict=False))\n",
    "            df=df.with_columns(casts) if casts else df\n",
    "            if \"symbol\" not in df.columns: df=df.with_columns([pl.lit(symbol).alias(\"symbol\")])\n",
    "            return df\n",
    "        except Exception:\n",
    "            if a<max_retries: time.sleep(sleep_base*(2**a)); a+=1; continue\n",
    "            return pl.DataFrame()\n",
    "\n",
    "# 3) Parallel fetch\n",
    "frames=[]\n",
    "if index_symbols:\n",
    "    max_workers=min(16, max(4, (os.cpu_count() or 8)))\n",
    "    with cf.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        for fut in cf.as_completed({ex.submit(fetch_index_quotes, s): s for s in index_symbols}):\n",
    "            df=fut.result(); frames.append(df) if isinstance(df,pl.DataFrame) and not df.is_empty() else None\n",
    "\n",
    "index_quotes_df=pl.DataFrame() if not frames else pl.concat(frames, how=\"vertical_relaxed\")\n",
    "\n",
    "# Unique by (symbol,date)\n",
    "if not index_quotes_df.is_empty():\n",
    "    keep=[c for c in index_quotes_df.columns if c in {\"symbol\",\"date\",\"price\",\"volume\"}] or index_quotes_df.columns\n",
    "    index_quotes_df=index_quotes_df.select(keep).unique(subset=[\"symbol\",\"date\"], keep=\"last\").sort([\"symbol\",\"date\"])\n",
    "\n",
    "print(index_quotes_df.shape); print(index_quotes_df.head())\n",
    "\n",
    "# 4) Load to DuckDB\n",
    "if not index_quotes_df.is_empty():\n",
    "    import duckdb\n",
    "    con=duckdb.connect('../americas.db'); con.sql(\"CREATE OR REPLACE TABLE index_quotes AS SELECT * FROM index_quotes_df\"); con.close()\n",
    "else:\n",
    "    print(\"No index stock_quotes fetched; skipping DuckDB load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f058f",
   "metadata": {},
   "source": [
    "## Verify database tables and record counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1fe7161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "  - exchanges: 14 rows\n",
      "  - index_list: 36 rows\n",
      "  - index_quotes: 111,558 rows\n",
      "  - risk_premium: 44 rows\n",
      "  - stock_metrics: 525,189 rows\n",
      "  - stock_profiles: 11,866 rows\n",
      "  - stock_quotes: 34,102,582 rows\n",
      "  - stock_tickers: 11,866 rows\n",
      "Latest quote date: 2025-09-17 00:00:00\n",
      "Latest index quote date: 2025-09-17\n"
     ]
    }
   ],
   "source": [
    "# Verify database tables and record counts\n",
    "import duckdb\n",
    "con = duckdb.connect('../americas.db')\n",
    "tables = con.sql(\"SHOW TABLES\").fetchall()\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    count = con.sql(f\"SELECT COUNT(*) AS count FROM {table[0]}\").fetchone()[0]\n",
    "    print(f\"  - {table[0]}: {count:,} rows\")\n",
    "\n",
    "# Check the latest dates in the stock_quotes and index_quotes tables\n",
    "if any(t[0] == 'stock_quotes' for t in tables):\n",
    "    last_quote_date = con.sql(\"SELECT MAX(date) FROM stock_quotes\").fetchone()[0]\n",
    "    print(f\"Latest quote date: {last_quote_date}\")\n",
    "\n",
    "if any(t[0] == 'index_quotes' for t in tables):\n",
    "    last_index_date = con.sql(\"SELECT MAX(date) FROM index_quotes\").fetchone()[0] \n",
    "    print(f\"Latest index quote date: {last_index_date}\")\n",
    "\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
