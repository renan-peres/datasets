{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68eb25f0",
   "metadata": {},
   "source": [
    "# Scrape Americas Stocks and Fundamentals -- Financial Modeling Prep (FMP) API\n",
    "\n",
    "End‑to‑end pipeline to build an Americas universe (stock_tickers + stock_profiles), fetch stock_quotes and key stock_metrics/ratios, and persist everything into DuckDB.\n",
    "\n",
    "- Data sources (FMP stable APIs):\n",
    "  - Exchanges: https://financialmodelingprep.com/stable/available-exchanges\n",
    "  - Stock list/stock_profiles (paged bulk): https://financialmodelingprep.com/stable/profile-bulk?part=0\n",
    "  - EOD bulk (daily): https://financialmodelingprep.com/stable/eod-bulk?date=YYYY-MM-DD\n",
    "  - Key stock_metrics: https://financialmodelingprep.com/stable/key-stock_metrics?symbol=AAPL&period=quarter&limit=100\n",
    "  - Ratios: https://financialmodelingprep.com/stable/ratios?symbol=AAPL&period=quarter&limit=100\n",
    "  - Index list: https://financialmodelingprep.com/stable/index-list\n",
    "  - Index quotes (light): https://financialmodelingprep.com/stable/historical-price-eod/light\n",
    "\n",
    "- Inputs\n",
    "  - Optional environment variable FMP_API_KEY to raise rate limits: export FMP_API_KEY=your_key\n",
    "\n",
    "- Outputs (DuckDB: americas.db)\n",
    "  - exchanges, stock_profiles, stock_tickers, stock_quotes, key_metrics, index_list, index_quotes\n",
    "\n",
    "- Run order\n",
    "  1) Setup and exchanges filter  2) Profiles (paged) → stock_tickers  3) Load stock_profiles\n",
    "  4) EOD bulk stock_quotes  5) Load stock_quotes  6) Key stock_metrics + ratios → load\n",
    "  7) Exchanges table  8) index_list + stock_quotes → load\n",
    "\n",
    "Notes\n",
    "- “Americas” classification uses FMP exchange metadata (region/country) and symbol suffix mapping; U.S. stock_tickers with no suffix are included.\n",
    "- Network calls use simple retry/backoff and thread pools to balance speed and rate limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71867fd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- Optionally set FMP_API_KEY in your shell to improve quotas: export FMP_API_KEY=your_key.\n",
    "- All outputs are persisted into DuckDB (americas.db). No static files are read or written by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62035ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API response type: <class 'list'>\n",
      "Processing list with 71 items\n",
      "Created SUFFIX_TO_EXCHANGE mapping with 65 entries\n",
      "Identified 14 Americas exchanges: AMEX, BUE, BVC, CBOE, CNQ, MEX, NASDAQ, NEO, NYSE, OTC, SAO, SGO, TSX, TSXV\n"
     ]
    }
   ],
   "source": [
    "import os, requests\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\")\n",
    "params={\"apikey\":API_KEY} if API_KEY else {}\n",
    "\n",
    "# Fetch exchange data with better error handling\n",
    "try:\n",
    "    raw=requests.get(\"https://financialmodelingprep.com/stable/available-exchanges\",params=params,timeout=30).json()\n",
    "    print(f\"API response type: {type(raw)}\")\n",
    "    \n",
    "    # Handle both potential response formats\n",
    "    if isinstance(raw, dict):\n",
    "        # Extract list from dictionary if possible\n",
    "        list_found = False\n",
    "        for k, v in raw.items():\n",
    "            if isinstance(v, list) and len(v) > 0:\n",
    "                raw = v\n",
    "                list_found = True\n",
    "                print(f\"Found list in key '{k}' with {len(v)} items\")\n",
    "                break\n",
    "        if not list_found:\n",
    "            print(\"No list found in response dictionary, using empty list\")\n",
    "            raw = []\n",
    "    elif not isinstance(raw, list):\n",
    "        print(f\"Unexpected response type: {type(raw)}, using empty list\")\n",
    "        raw = []\n",
    "        \n",
    "    # Log response size for debugging\n",
    "    if isinstance(raw, list):\n",
    "        print(f\"Processing list with {len(raw)} items\")\n",
    "except Exception as e:\n",
    "    print(f\"API request failed: {e}\")\n",
    "    raw = []\n",
    "\n",
    "ex_data = raw\n",
    "SUFFIX_TO_EXCHANGE = {}\n",
    "if isinstance(ex_data, list) and len(ex_data) > 0:\n",
    "    SUFFIX_TO_EXCHANGE = { \n",
    "        (i.get(\"symbolSuffix\") or \"\").strip().upper(): (i.get(\"exchange\") or \"\").strip().upper() \n",
    "        for i in ex_data \n",
    "        if (i.get(\"symbolSuffix\") or \"\").strip() and (i.get(\"exchange\") or \"\").strip() \n",
    "    }\n",
    "    print(f\"Created SUFFIX_TO_EXCHANGE mapping with {len(SUFFIX_TO_EXCHANGE)} entries\")\n",
    "\n",
    "# Infer Americas exchanges from API (no file reads)\n",
    "CTRY={\"UNITED STATES\",\"CANADA\",\"BRAZIL\",\"MEXICO\",\"ARGENTINA\",\"CHILE\",\"PERU\",\"COLOMBIA\",\"VENEZUELA\",\"URUGUAY\",\"PARAGUAY\",\"BOLIVIA\",\"ECUADOR\",\"GUYANA\",\"SURINAME\",\"FRENCH GUIANA\",\"JAMAICA\",\"TRINIDAD AND TOBAGO\",\"TRINIDAD & TOBAGO\",\"BARBADOS\",\"BAHAMAS\",\"BERMUDA\",\"CAYMAN ISLANDS\",\"PANAMA\",\"COSTA RICA\",\"GUATEMALA\",\"HONDURAS\",\"EL SALVADOR\",\"NICARAGUA\",\"DOMINICAN REPUBLIC\",\"HAITI\",\"PUERTO RICO\",\"BELIZE\",\"CURACAO\",\"ARUBA\",\"SAINT LUCIA\",\"GRENADA\",\"ST. VINCENT AND THE GRENADINES\"}\n",
    "ISO2={\"US\",\"CA\",\"BR\",\"MX\",\"AR\",\"CL\",\"PE\",\"CO\",\"VE\",\"UY\",\"PY\",\"BO\",\"EC\",\"GY\",\"SR\",\"GF\",\"JM\",\"TT\",\"BB\",\"BS\",\"BM\",\"KY\",\"PA\",\"CR\",\"GT\",\"HN\",\"SV\",\"NI\",\"DO\",\"H\",\"PR\",\"BZ\",\"LC\",\"GD\",\"VC\",\"CW\",\"AW\"}\n",
    "\n",
    "def _amer_exchange(rec: dict)->bool:\n",
    "    for k in (\"region\",\"continent\"):\n",
    "        v=rec.get(k)\n",
    "        if isinstance(v,str) and \"AMERICA\" in v.upper(): return True\n",
    "    for k in (\"country\",\"countryName\",\"countryCode\",\"country_code\",\"country_iso2\"):\n",
    "        v=rec.get(k); vu=v.strip().upper() if isinstance(v,str) else \"\"\n",
    "        if vu and ((len(vu)<=3 and vu in ISO2) or vu in CTRY or \"UNITED STATES\" in vu or \"LATIN AMERICA\" in vu): return True\n",
    "    return False\n",
    "\n",
    "EXCHANGES_AMERICAS=set()\n",
    "for it in ex_data:\n",
    "    if _amer_exchange(it):\n",
    "        exch=(it.get(\"exchange\") or \"\").strip(); acr=(it.get(\"acronym\") or \"\").strip(); mic=(it.get(\"mic\") or \"\").strip()\n",
    "        if exch: EXCHANGES_AMERICAS.add(exch.upper())\n",
    "        elif acr or mic: EXCHANGES_AMERICAS.add((acr or mic).upper())\n",
    "\n",
    "# Fallback: infer from known U.S./Americas exchange acronyms in the exchange name map\n",
    "if not EXCHANGES_AMERICAS:\n",
    "    for _, exch in SUFFIX_TO_EXCHANGE.items():\n",
    "        eu=exch.upper()\n",
    "        if any(x in eu for x in (\"NAS\",\"NYS\",\"ARC\",\"BATS\",\"OTC\",\"TSX\",\"TSXV\",\"CSE\",\"BOV\",\"MEX\",\"XBOV\",\"XMEX\")):\n",
    "            EXCHANGES_AMERICAS.add(eu)\n",
    "\n",
    "print(f\"Identified {len(EXCHANGES_AMERICAS)} Americas exchanges: {', '.join(sorted(EXCHANGES_AMERICAS))}\")\n",
    "if not EXCHANGES_AMERICAS:\n",
    "    # Use hardcoded fallback instead of raising error to allow continuing execution\n",
    "    print(\"WARNING: Could not infer any Americas exchanges from API payload. Using hardcoded fallbacks.\")\n",
    "    EXCHANGES_AMERICAS = {\"NYSE\",\"NASDAQ\",\"AMEX\",\"ARCX\",\"NYS\",\"NAS\",\"ARC\",\"BATS\",\"OTC\",\"TSX\",\"TSXV\",\"CSE\"}\n",
    "    print(f\"Using fallback exchanges: {', '.join(sorted(EXCHANGES_AMERICAS))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b09791",
   "metadata": {},
   "source": [
    "## Profiles (paged) → Universe\n",
    "\n",
    "Pull paged company profiles and keep only Americas listings (via symbol suffix→exchange map). Limit the investable universe to marketCap ≥ 1B and persist to DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "039a481f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11801, 36)\n",
      "shape: (5, 36)\n",
      "┌─────────────┬──────────┬────────┬─────────┬───┬─────────────┬─────────────┬────────┬─────────────┐\n",
      "│ isActivelyT ┆ currency ┆ isFund ┆ price   ┆ … ┆ image       ┆ isin        ┆ symbol ┆ website     │\n",
      "│ rading      ┆ ---      ┆ ---    ┆ ---     ┆   ┆ ---         ┆ ---         ┆ ---    ┆ ---         │\n",
      "│ ---         ┆ str      ┆ str    ┆ f64     ┆   ┆ str         ┆ str         ┆ str    ┆ str         │\n",
      "│ str         ┆          ┆        ┆         ┆   ┆             ┆             ┆        ┆             │\n",
      "╞═════════════╪══════════╪════════╪═════════╪═══╪═════════════╪═════════════╪════════╪═════════════╡\n",
      "│ true        ┆ USD      ┆ false  ┆ 0.89    ┆ … ┆ https://ima ┆ SG1V1293623 ┆ SRHBF  ┆ https://www │\n",
      "│             ┆          ┆        ┆         ┆   ┆ ges.financi ┆ 2           ┆        ┆ .starhub.co │\n",
      "│             ┆          ┆        ┆         ┆   ┆ almodeli…   ┆             ┆        ┆ m           │\n",
      "│ true        ┆ USD      ┆ false  ┆ 14.5125 ┆ … ┆ https://ima ┆ US633643879 ┆ NBGRY  ┆ https://www │\n",
      "│             ┆          ┆        ┆         ┆   ┆ ges.financi ┆ 0           ┆        ┆ .nbg.gr     │\n",
      "│             ┆          ┆        ┆         ┆   ┆ almodeli…   ┆             ┆        ┆             │\n",
      "│ true        ┆ USD      ┆ false  ┆ 84.75   ┆ … ┆ https://ima ┆ US384109104 ┆ GGG    ┆ https://www │\n",
      "│             ┆          ┆        ┆         ┆   ┆ ges.financi ┆ 0           ┆        ┆ .graco.com  │\n",
      "│             ┆          ┆        ┆         ┆   ┆ almodeli…   ┆             ┆        ┆             │\n",
      "│ true        ┆ USD      ┆ false  ┆ 41.5    ┆ … ┆ https://ima ┆ US381430107 ┆ GSIE   ┆ null        │\n",
      "│             ┆          ┆        ┆         ┆   ┆ ges.financi ┆ 9           ┆        ┆             │\n",
      "│             ┆          ┆        ┆         ┆   ┆ almodeli…   ┆             ┆        ┆             │\n",
      "│ true        ┆ USD      ┆ false  ┆ 225.33  ┆ … ┆ https://ima ┆ US760759100 ┆ RSG    ┆ https://www │\n",
      "│             ┆          ┆        ┆         ┆   ┆ ges.financi ┆ 2           ┆        ┆ .republicse │\n",
      "│             ┆          ┆        ┆         ┆   ┆ almodeli…   ┆             ┆        ┆ rvices.c…   │\n",
      "└─────────────┴──────────┴────────┴─────────┴───┴─────────────┴─────────────┴────────┴─────────────┘\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, requests, polars as pl\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\")\n",
    "\n",
    "def fetch_profiles_paged(api_key: str|None=None, start_part: int=0, max_parts: int|None=None, sleep_s: float=0.0, max_retries: int=3, verbose: bool=False)->pl.DataFrame:\n",
    "    key=api_key or API_KEY; params={\"apikey\":key} if key else {}\n",
    "    # ensure exchange context\n",
    "    global SUFFIX_TO_EXCHANGE, EXCHANGES_AMERICAS\n",
    "    if 'SUFFIX_TO_EXCHANGE' not in globals() or 'EXCHANGES_AMERICAS' not in globals():\n",
    "        try:\n",
    "            ex_data=requests.get(\"https://financialmodelingprep.com/stable/available-exchanges\",params=params,timeout=60).json()\n",
    "            ex_data = ex_data if isinstance(ex_data,list) else []\n",
    "        except Exception: ex_data=[]\n",
    "        SUFFIX_TO_EXCHANGE = 'SUFFIX_TO_EXCHANGE' in globals() and SUFFIX_TO_EXCHANGE or { (i.get(\"symbolSuffix\") or \"\").strip().upper(): (i.get(\"exchange\") or \"\").strip().upper() for i in ex_data if (i.get(\"symbolSuffix\") or \"\").strip() and (i.get(\"exchange\") or \"\").strip() }\n",
    "        # Infer amer exchanges from payload directly (no file reads)\n",
    "        CTRY={\"UNITED STATES\",\"CANADA\",\"BRAZIL\",\"MEXICO\",\"ARGENTINA\",\"CHILE\",\"PERU\",\"COLOMBIA\",\"VENEZUELA\",\"URUGUAY\",\"PARAGUAY\",\"BOLIVIA\",\"ECUADOR\",\"GUYANA\",\"SURINAME\",\"FRENCH GUIANA\",\"JAMAICA\",\"TRINIDAD AND TOBAGO\",\"TRINIDAD & TOBAGO\",\"BARBADOS\",\"BAHAMAS\",\"BERMUDA\",\"CAYMAN ISLANDS\",\"PANAMA\",\"COSTA RICA\",\"GUATEMALA\",\"HONDURAS\",\"EL SALVADOR\",\"NICARAGUA\",\"DOMINICAN REPUBLIC\",\"HAITI\",\"PUERTO RICO\",\"BELIZE\",\"CURACAO\",\"ARUBA\",\"SAINT LUCIA\",\"GRENADA\",\"ST. VINCENT AND THE GRENADINES\"}\n",
    "        ISO2={\"US\",\"CA\",\"BR\",\"MX\",\"AR\",\"CL\",\"PE\",\"CO\",\"VE\",\"UY\",\"PY\",\"BO\",\"EC\",\"GY\",\"SR\",\"GF\",\"JM\",\"TT\",\"BB\",\"BS\",\"BM\",\"KY\",\"PA\",\"CR\",\"GT\",\"HN\",\"SV\",\"NI\",\"DO\",\"H\",\"PR\",\"BZ\",\"LC\",\"GD\",\"VC\",\"CW\",\"AW\"}\n",
    "        def amer(r):\n",
    "            if any(isinstance(r.get(k), str) and \"AMERICA\" in r[k].upper() for k in (\"region\",\"continent\")): return True\n",
    "            for k in (\"country\",\"countryCode\",\"country_code\",\"country_iso2\",\"countryName\"):\n",
    "                v=r.get(k); vu=v.strip().upper() if isinstance(v,str) else \"\"\n",
    "                if vu and ((len(vu)<=3 and vu in ISO2) or vu in CTRY or \"UNITED STATES\" in vu or \"LATIN AMERICA\" in vu): return True\n",
    "            return False\n",
    "        EXCHANGES_AMERICAS=set()\n",
    "        for it in ex_data:\n",
    "            if amer(it):\n",
    "                exch=(it.get(\"exchange\") or \"\").strip(); acr=(it.get(\"acronym\") or \"\").strip(); mic=(it.get(\"mic\") or \"\").strip()\n",
    "                if exch: EXCHANGES_AMERICAS.add(exch.upper())\n",
    "                elif acr or mic: EXCHANGES_AMERICAS.add((acr or mic).upper())\n",
    "        if not EXCHANGES_AMERICAS:\n",
    "            for _, exch in SUFFIX_TO_EXCHANGE.items():\n",
    "                eu=str(exch).upper()\n",
    "                if any(x in eu for x in (\"NAS\",\"NYS\",\"ARC\",\"BATS\",\"OTC\",\"TSX\",\"TSXV\",\"CSE\",\"BOV\",\"MEX\",\"XBOV\",\"XMEX\")):\n",
    "                    EXCHANGES_AMERICAS.add(eu)\n",
    "    _is_amer=lambda s: isinstance(s,str) and (SUFFIX_TO_EXCHANGE.get((\".\"+s.rsplit(\".\",1)[-1]).upper()) in EXCHANGES_AMERICAS if \".\" in s else True)\n",
    "\n",
    "    def _parse(txt: str)->list[dict]:\n",
    "        s=txt.lstrip()\n",
    "        if s.startswith('['):\n",
    "            try: return json.loads(s)\n",
    "            except Exception: pass\n",
    "        norm=txt.replace('}{','}\\n{'); rec=[]\n",
    "        for ln in (l.strip() for l in norm.splitlines() if l.strip()):\n",
    "            i=ln.find('{'); ln=ln[i:] if i>=0 else ln\n",
    "            if ln.startswith('{'):\n",
    "                try: o=json.loads(ln); rec.append(o) if isinstance(o,dict) else None\n",
    "                except Exception: pass\n",
    "        if rec: return rec\n",
    "        if ',' in txt and '\\n' in txt:\n",
    "            try: return pl.read_csv(StringIO(txt), ignore_errors=True).to_dicts()\n",
    "            except Exception: pass\n",
    "            try:\n",
    "                import pandas as pd; return pl.from_pandas(pd.read_csv(StringIO(txt), engine=\"python\", dtype=str, on_bad_lines=\"skip\").fillna(\"\")).to_dicts()\n",
    "            except Exception: pass\n",
    "            try:\n",
    "                import csv; return [dict({k:(v or \"\") for k,v in r.items()}) for r in csv.DictReader(StringIO(txt))]\n",
    "            except Exception: return []\n",
    "        return []\n",
    "\n",
    "    def _to_pl(records: list[dict])->pl.DataFrame:\n",
    "        if not records: return pl.DataFrame()\n",
    "        keys=list({k for r in records for k in r.keys()}); norm=[{k:r.get(k,None) for k in keys} for r in records]\n",
    "        try: return pl.DataFrame(norm, schema_overrides={k:pl.Utf8 for k in keys}, strict=False, infer_schema_length=len(norm))\n",
    "        except Exception:\n",
    "            try:\n",
    "                import pandas as pd; return pl.from_pandas(pd.DataFrame(norm).astype(\"string\").fillna(\"\"))\n",
    "            except Exception: return pl.DataFrame()\n",
    "\n",
    "    sess=requests.Session(); url=\"https://financialmodelingprep.com/stable/profile-bulk\"; frames=[]; part=start_part\n",
    "    while True:\n",
    "        if max_parts is not None and part>=start_part+max_parts: break\n",
    "        q={\"part\":part,\"datatype\":\"json\",**params}; attempt=0\n",
    "        while True:\n",
    "            try:\n",
    "                r=sess.get(url,params=q,timeout=120); status=r.status_code\n",
    "                if verbose: print(f\"GET {r.url[:80]}... -> {status}\")\n",
    "                if status in (429,) or status>=500:\n",
    "                    if attempt<max_retries: time.sleep((sleep_s or 0.5)*(2**attempt)); attempt+=1; continue\n",
    "                if status==403: return pl.DataFrame()\n",
    "                r.raise_for_status(); data=_parse(r.text)\n",
    "            except Exception:\n",
    "                return pl.DataFrame() if not frames else pl.concat(frames, how=\"vertical_relaxed\").unique(subset=[\"symbol\"], keep=\"last\")\n",
    "            break\n",
    "        if not isinstance(data,list): break\n",
    "        if not data:\n",
    "            if r.text.strip(): part+=1; time.sleep(sleep_s) if sleep_s>0 else None; continue\n",
    "            break\n",
    "        df=_to_pl(data)\n",
    "        if df.is_empty(): part+=1; continue\n",
    "        if \"Symbol\" in df.columns and \"symbol\" not in df.columns: df=df.rename({\"Symbol\":\"symbol\"})\n",
    "        df=df.filter(pl.col(\"symbol\").map_elements(_is_amer, return_dtype=pl.Boolean)) if \"symbol\" in df.columns else pl.DataFrame()\n",
    "        if not df.is_empty():\n",
    "            casts=[]\n",
    "            for c in (\"price\",\"beta\",\"lastDiv\",\"lastDividend\",\"change\",\"changes\",\"changePercentage\",\"changesPercentage\"): casts.append(pl.col(c).cast(pl.Float64, strict=False)) if c in df.columns else None\n",
    "            for c in (\"marketCap\",\"mktCap\",\"volume\",\"volAvg\",\"avgVolume\",\"averageVolume\"): casts.append(pl.col(c).cast(pl.Int64, strict=False)) if c in df.columns else None\n",
    "            df=df.with_columns(casts) if casts else df; frames.append(df)\n",
    "        part+=1; time.sleep(sleep_s) if sleep_s>0 else None\n",
    "    return pl.DataFrame() if not frames else pl.concat(frames, how=\"vertical_relaxed\").unique(subset=[\"symbol\"], keep=\"last\")\n",
    "\n",
    "profiles_df=fetch_profiles_paged(api_key=os.getenv(\"FMP_API_KEY\"))\n",
    "profiles_df=profiles_df.filter(pl.col(\"marketCap\").cast(pl.Int64, strict=False) >= 1_000_000_000)\n",
    "stock_tickers = profiles_df.select(\"symbol\").unique().sort(\"symbol\")\n",
    "print(profiles_df.shape)\n",
    "print(profiles_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e1698",
   "metadata": {},
   "source": [
    "### Save tickers → DuckDB\n",
    "Persist unique investable symbols into americas.db.tickers for downstream joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bd1492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, polars as pl\n",
    "# Incremental load for stock_tickers (unique by symbol)\n",
    "if 'stock_tickers' in globals() and isinstance(stock_tickers, pl.DataFrame) and not stock_tickers.is_empty():\n",
    "    con = duckdb.connect('../americas.db')\n",
    "    # Register incoming dataframe\n",
    "    con.register('tickers_view', stock_tickers.to_pandas())\n",
    "    # Create table if it does not exist (empty schema clone)\n",
    "    con.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS stock_tickers AS\n",
    "        SELECT * FROM tickers_view LIMIT 0\n",
    "    \"\"\")\n",
    "    # Insert only new symbols\n",
    "    con.sql(\"\"\"\n",
    "        INSERT INTO stock_tickers\n",
    "        SELECT t.*\n",
    "        FROM tickers_view t\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1 FROM stock_tickers x WHERE x.symbol = t.symbol\n",
    "        )\n",
    "    \"\"\")\n",
    "    # (Optional) collect count of new rows inserted in this run\n",
    "    new_count = con.sql(\"SELECT COUNT(*) AS c FROM tickers_view WHERE symbol NOT IN (SELECT symbol FROM stock_tickers)\").fetchone()[0] if False else None\n",
    "    con.close()\n",
    "else:\n",
    "    print(\"No stock_tickers to save; skipping DuckDB load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9716a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb, polars as pl\n",
    "# Incremental load for stock_profiles (unique by symbol) with robust dynamic casting\n",
    "if 'profiles_df' in globals() and isinstance(profiles_df, pl.DataFrame) and not profiles_df.is_empty():\n",
    "    cap_col = 'marketCap' if 'marketCap' in profiles_df.columns else ('mktCap' if 'mktCap' in profiles_df.columns else None)\n",
    "    if cap_col:\n",
    "        # Filter investable universe\n",
    "        filtered = profiles_df.filter(pl.col(cap_col).cast(pl.Int64, strict=False) >= 1_000_000_000)\n",
    "        if not filtered.is_empty():\n",
    "            con = duckdb.connect('../americas.db')\n",
    "            table_exists = False\n",
    "            try:\n",
    "                table_exists = bool(con.execute(\"SELECT 1 FROM information_schema.tables WHERE table_name='stock_profiles'\").fetchone())\n",
    "            except Exception:\n",
    "                table_exists = False\n",
    "\n",
    "            if not table_exists:\n",
    "                # Detect boolean-like columns (string reps of true/false only)\n",
    "                bool_like = []\n",
    "                for c in filtered.columns:\n",
    "                    try:\n",
    "                        vals = filtered.select(pl.col(c).cast(pl.Utf8, strict=False).str.to_lowercase().drop_nulls().unique()).to_series().to_list()\n",
    "                        if vals and all(v in ('true','false') for v in vals):\n",
    "                            bool_like.append(c)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                if bool_like:\n",
    "                    casts = [\n",
    "                        pl.when(pl.col(c).cast(pl.Utf8, strict=False).str.to_lowercase()==\"true\").then(pl.lit(1))\n",
    "                          .when(pl.col(c).cast(pl.Utf8, strict=False).str.to_lowercase()==\"false\").then(pl.lit(0))\n",
    "                          .otherwise(pl.lit(None)).alias(c)\n",
    "                        for c in bool_like\n",
    "                    ]\n",
    "                    filtered = filtered.with_columns(casts)\n",
    "                # Basic numeric casts for common fields\n",
    "                float_cols = [c for c in (\"price\",\"beta\",\"lastDiv\",\"lastDividend\",\"change\",\"changes\",\"changePercentage\",\"changesPercentage\") if c in filtered.columns]\n",
    "                int_cols = [c for c in (\"marketCap\",\"mktCap\",\"volume\",\"volAvg\",\"avgVolume\",\"averageVolume\") if c in filtered.columns]\n",
    "                casts = [pl.col(c).cast(pl.Float64, strict=False) for c in float_cols] + [pl.col(c).cast(pl.Int64, strict=False) for c in int_cols]\n",
    "                if casts:\n",
    "                    filtered = filtered.with_columns(casts)\n",
    "                # Create table schema clone + initial load\n",
    "                con.register('profiles_incoming_initial', filtered.to_pandas())\n",
    "                con.sql(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS stock_profiles AS\n",
    "                    SELECT * FROM profiles_incoming_initial LIMIT 0\n",
    "                \"\"\")\n",
    "                con.sql(\"INSERT INTO stock_profiles SELECT * FROM profiles_incoming_initial\")\n",
    "                con.close()\n",
    "            else:\n",
    "                # Existing table: build insert aligned to destination schema while avoiding lower() on numeric sources\n",
    "                con.register('profiles_incoming_raw', filtered.to_pandas())\n",
    "                schema_rows = con.execute(\"PRAGMA table_info('stock_profiles')\").fetchall()\n",
    "                # Map source column polars dtypes to simple strings\n",
    "                src_type_map = {c: str(t) for c, t in zip(filtered.columns, filtered.dtypes)}\n",
    "                numeric_prefixes = (\"Int\",\"UInt\",\"Float\",\"Decimal\")\n",
    "                select_exprs = []\n",
    "                dest_cols = []\n",
    "                for cid, name, dtype, *_ in schema_rows:\n",
    "                    dest_cols.append(name)\n",
    "                    upper_type = (dtype or '').upper()\n",
    "                    if name == 'symbol':\n",
    "                        expr = f\"p.{name} AS {name}\"\n",
    "                    elif name in filtered.columns:\n",
    "                        src_t = src_type_map.get(name, \"\")\n",
    "                        is_src_numeric = any(src_t.startswith(pref) for pref in numeric_prefixes)\n",
    "                        if 'INT' in upper_type:\n",
    "                            if is_src_numeric:\n",
    "                                # Source already numeric -> direct cast\n",
    "                                expr = f\"TRY_CAST(p.{name} AS {upper_type}) AS {name}\"\n",
    "                            else:\n",
    "                                # Source textual -> handle boolean-like strings then try numeric cast\n",
    "                                expr = (\n",
    "                                    f\"CASE WHEN lower(CAST(p.{name} AS VARCHAR))='true' THEN 1 \"\n",
    "                                    f\"WHEN lower(CAST(p.{name} AS VARCHAR))='false' THEN 0 \"\n",
    "                                    f\"ELSE TRY_CAST(p.{name} AS {upper_type}) END AS {name}\"\n",
    "                                )\n",
    "                        elif any(t in upper_type for t in ['DOUBLE','FLOAT','REAL','DECIMAL']):\n",
    "                            expr = f\"TRY_CAST(p.{name} AS {upper_type}) AS {name}\"\n",
    "                        else:\n",
    "                            # Leave as-is\n",
    "                            expr = f\"p.{name} AS {name}\"\n",
    "                    else:\n",
    "                        null_cast_type = upper_type if upper_type else 'VARCHAR'\n",
    "                        expr = f\"CAST(NULL AS {null_cast_type}) AS {name}\"\n",
    "                    select_exprs.append(expr)\n",
    "                insert_sql = f\"\"\"\n",
    "                    INSERT INTO stock_profiles ({','.join(dest_cols)})\n",
    "                    SELECT {','.join(select_exprs)}\n",
    "                    FROM profiles_incoming_raw p\n",
    "                    WHERE NOT EXISTS (\n",
    "                        SELECT 1 FROM stock_profiles x WHERE x.symbol = p.symbol\n",
    "                    )\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    con.sql(insert_sql)\n",
    "                except Exception as e:\n",
    "                    print(f\"Incremental insert failed: {e}\")\n",
    "                con.close()\n",
    "        else:\n",
    "            print(\"No stock_profiles with marketCap >= 1,000,000,000; skipping DuckDB load\")\n",
    "    else:\n",
    "        print(\"No market cap column found; skipping DuckDB load for stock_profiles\")\n",
    "else:\n",
    "    print(\"profiles_df is empty or undefined; skipping DuckDB load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f115d6",
   "metadata": {},
   "source": [
    "## EOD Bulk Quotes (2010‑01‑01 → today)\n",
    "\n",
    "Parallel fetch daily bulk EOD, filter to Americas + investable stock_tickers, and stage for DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3eff3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel EOD Bulk (full or incremental range), filtered by `stock_tickers` + logging\n",
    "import os, requests, polars as pl, concurrent.futures as cf, logging, threading\n",
    "from io import StringIO\n",
    "from datetime import date as _date, timedelta\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "log = logging.getLogger(\"eod\")\n",
    "\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\"); params={\"apikey\":API_KEY} if API_KEY else {}\n",
    "SESSION = requests.Session()\n",
    "# Enlarge connection pool to avoid 'Connection pool is full' warnings under concurrency\n",
    "try:\n",
    "    ADAPTER = HTTPAdapter(pool_connections=128, pool_maxsize=128)\n",
    "    SESSION.mount('https://', ADAPTER); SESSION.mount('http://', ADAPTER)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Ensure filter context exists even if setup cells weren't run\n",
    "if 'SUFFIX_TO_EXCHANGE' not in globals(): SUFFIX_TO_EXCHANGE = {}\n",
    "if 'EXCHANGES_AMERICAS' not in globals(): EXCHANGES_AMERICAS = set()\n",
    "\n",
    "# business-day date range\n",
    "def date_range(start: str, end: str, weekdays_only: bool=True):\n",
    "    y,m,d=map(int,start.split(\"-\")); ye,me,de=map(int,end.split(\"-\")); dt=_date(y,m,d); end_dt=_date(ye,me,de)\n",
    "    while dt<=end_dt:\n",
    "        if not weekdays_only or dt.weekday()<5: yield dt.isoformat()\n",
    "        dt+=timedelta(days=1)\n",
    "\n",
    "_is_amer=lambda s: isinstance(s,str) and (SUFFIX_TO_EXCHANGE.get((\".\"+s.rsplit(\".\",1)[-1]).upper()) in EXCHANGES_AMERICAS if \".\" in s else True)\n",
    "\n",
    "# daily fetch with retry/backoff\n",
    "def fetch_one_day(ds:str, allowed_symbols: set[str]|None=None, max_retries:int=3)->pl.DataFrame:\n",
    "    attempt=0\n",
    "    while True:\n",
    "        try:\n",
    "            r=SESSION.get(f\"https://financialmodelingprep.com/stable/eod-bulk?date={ds}\", params=params, timeout=120); status=r.status_code\n",
    "            if status in (429,) or status>=500:\n",
    "                if attempt<max_retries:\n",
    "                    import time; log.warning(f\"{ds} -> {status}, retry {attempt+1}/{max_retries}\")\n",
    "                    time.sleep(0.5*(2**attempt)); attempt+=1; continue\n",
    "            r.raise_for_status()\n",
    "            # Read potentially mixed-type numeric columns as strings to avoid parse errors, then cast below\n",
    "            df=pl.read_csv(\n",
    "                StringIO(r.text),\n",
    "                try_parse_dates=False,\n",
    "                schema_overrides={\"open\": pl.Utf8, \"high\": pl.Utf8, \"low\": pl.Utf8, \"close\": pl.Utf8, \"adjClose\": pl.Utf8, \"volume\": pl.Utf8},\n",
    "                infer_schema_length=1000,\n",
    "            )\n",
    "            if df.is_empty(): log.debug(f\"{ds} -> 0 rows\"); return df\n",
    "            n0=len(df); df=df.filter(pl.col(\"symbol\").map_elements(_is_amer, return_dtype=pl.Boolean)); n1=len(df)\n",
    "            if df.is_empty(): log.debug(f\"{ds} -> amer 0/{n0}\"); return df\n",
    "            if allowed_symbols: df=df.filter(pl.col(\"symbol\").is_in(allowed_symbols)); n2=len(df)\n",
    "            else: n2=n1\n",
    "            log.debug(f\"{ds} -> raw={n0} amer={n1} allowed={n2}\")\n",
    "            return df.with_columns([\n",
    "                pl.col(\"date\").cast(pl.Utf8).str.to_date(\"%Y-%m-%d\"),\n",
    "                pl.col(\"open\").cast(pl.Float64, strict=False), pl.col(\"high\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"low\").cast(pl.Float64, strict=False), pl.col(\"close\").cast(pl.Float64, strict=False),\n",
    "                pl.col(\"adjClose\").cast(pl.Float64, strict=False),\n",
    "                # Cast volume via Float -> rounded Int to handle occasional fractional values\n",
    "                pl.col(\"volume\").cast(pl.Float64, strict=False).round(0).cast(pl.Int64, strict=False)\n",
    "            ])\n",
    "        except Exception as e:\n",
    "            if attempt<max_retries:\n",
    "                import time; log.warning(f\"{ds} -> error {e!r}, retry {attempt+1}/{max_retries}\")\n",
    "                time.sleep(0.5*(2**attempt)); attempt+=1; continue\n",
    "            log.error(f\"{ds} -> failed after {max_retries} retries: {e!r}\")\n",
    "            return pl.DataFrame()\n",
    "\n",
    "# Parallel over full or supplied range, with optional streaming insert\n",
    "def parallel_fetch_eod_bulk_range(start_date:str, end_date:str,\n",
    "                                  max_workers:int|None=None,\n",
    "                                  weekdays_only:bool=True,\n",
    "                                  allowed_symbols: set[str] | None = None,\n",
    "                                  days: list[str] | None = None,\n",
    "                                  skip_existing: bool = True,\n",
    "                                  existing_dates: set[str] | None = None,\n",
    "                                  insert_into_duckdb: bool = False,\n",
    "                                  duckdb_path: str = '../americas.db') -> pl.DataFrame:\n",
    "    \"\"\"Fetch EOD bulk data.\n",
    "    Optimizations:\n",
    "      - pass precomputed days list\n",
    "      - skip days already in stock_quotes table (existing_dates)\n",
    "      - optionally stream each fetched day into DuckDB (idempotent insert on (date,symbol))\n",
    "    \"\"\"\n",
    "    all_days = days if days is not None else list(date_range(start_date, end_date, weekdays_only=weekdays_only))\n",
    "    if skip_existing and existing_dates:\n",
    "        fetch_days=[d for d in all_days if d not in existing_dates]\n",
    "    else:\n",
    "        fetch_days=all_days\n",
    "    if not fetch_days:\n",
    "        log.info(\"No new days to fetch (incremental). Returning empty DataFrame.\")\n",
    "        return pl.DataFrame()\n",
    "    if max_workers is None:\n",
    "        import os as _os; max_workers=min(32, max(8, (_os.cpu_count() or 8)*2))\n",
    "    log.info(f\"EOD bulk {fetch_days[0]}→{fetch_days[-1]}: {len(fetch_days)} new days (of {len(all_days)} total), workers={max_workers}, symbols={'all' if not allowed_symbols else len(allowed_symbols)}\")\n",
    "\n",
    "    con = None\n",
    "    lock = threading.Lock()\n",
    "    if insert_into_duckdb:\n",
    "        import duckdb\n",
    "        con = duckdb.connect(duckdb_path)\n",
    "        # Ensure table exists\n",
    "        con.execute(\"CREATE TABLE IF NOT EXISTS stock_quotes (date DATE, symbol VARCHAR, open DOUBLE, high DOUBLE, low DOUBLE, close DOUBLE, adjClose DOUBLE, volume BIGINT)\")\n",
    "        # Create composite index (DuckDB 1.0 lacks indexes; rely on NOT EXISTS checks later)\n",
    "\n",
    "    frames=[]; done=0; step=max(1, len(fetch_days)//20)\n",
    "    with cf.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs={ex.submit(fetch_one_day, ds, allowed_symbols): ds for ds in fetch_days}\n",
    "        for fut in cf.as_completed(futs):\n",
    "            ds=futs[fut]\n",
    "            f=fut.result(); done+=1\n",
    "            if isinstance(f, pl.DataFrame) and not f.is_empty():\n",
    "                if insert_into_duckdb and con is not None:\n",
    "                    # Insert only new (date,symbol)\n",
    "                    try:\n",
    "                        import duckdb\n",
    "                        with lock:\n",
    "                            con.register('__new_day', f.to_pandas())\n",
    "                            con.execute(\"\"\"\n",
    "                                INSERT INTO stock_quotes\n",
    "                                SELECT n.* FROM __new_day n\n",
    "                                WHERE NOT EXISTS (\n",
    "                                    SELECT 1 FROM stock_quotes q WHERE q.date = n.date AND q.symbol = n.symbol\n",
    "                                )\n",
    "                            \"\"\")\n",
    "                            con.unregister('__new_day')\n",
    "                    except Exception as e:\n",
    "                        log.warning(f\"DuckDB insert failed for {ds}: {e}\")\n",
    "                else:\n",
    "                    frames.append(f)\n",
    "            if done%step==0 or done==len(fetch_days): log.info(f\"Progress {done}/{len(fetch_days)} days (fetched {len(frames)} non-empty frames)\")\n",
    "    if con is not None:\n",
    "        con.close()\n",
    "    out = pl.DataFrame() if not frames else pl.concat(frames, how=\"vertical_relaxed\").unique(subset=[\"date\",\"symbol\"], keep=\"last\").sort([\"date\",\"symbol\"])\n",
    "    log.info(f\"Combined rows (not counting already inserted days): {0 if out.is_empty() else out.height}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575560f",
   "metadata": {},
   "source": [
    "### Extract Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d65c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database: [('exchanges',), ('index_list',), ('index_quotes',), ('risk_premium',), ('stock_metrics',), ('stock_profiles',), ('stock_quotes',), ('stock_tickers',)]\n",
      "Fetching EOD bulk from 2025-09-16 to 2025-09-16\n"
     ]
    }
   ],
   "source": [
    "import duckdb as db\n",
    "from datetime import date  # or: from datetime import date as _date\n",
    "\n",
    "con = db.connect('../americas.db')\n",
    "\n",
    "# list tables\n",
    "tables = con.sql(\"SHOW TABLES\").fetchall()\n",
    "print(\"Tables in the database:\", tables)\n",
    "\n",
    "# get last quote date if table exists\n",
    "max_date = None\n",
    "if any(t[0] == 'stock_quotes' for t in tables):\n",
    "    max_date = con.sql(\"SELECT MAX(date) AS max_date FROM stock_quotes\").fetchone()[0]\n",
    "\n",
    "# Ensure string ISO format (DuckDB may return date/datetime object)\n",
    "if max_date:\n",
    "    # If you want to resume AFTER last stored day uncomment next line and import timedelta:\n",
    "    # from datetime import timedelta; max_date = (max_date + timedelta(days=1))\n",
    "    start_date = (max_date.date().isoformat() if hasattr(max_date, \"date\") else max_date.isoformat()) if hasattr(max_date, \"isoformat\") else str(max_date)\n",
    "else:\n",
    "    start_date = '2010-01-01'\n",
    "\n",
    "end_date = date.today().isoformat()\n",
    "print(f\"Fetching EOD bulk from {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c8e5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing quote days: 4098\n"
     ]
    }
   ],
   "source": [
    "# Gather existing quote dates to enable incremental skipping\n",
    "import duckdb as _db\n",
    "_existing_dates=set()\n",
    "try:\n",
    "    _con=_db.connect('../americas.db')\n",
    "    if _con.execute(\"SELECT 1 FROM information_schema.tables WHERE table_name='stock_quotes'\").fetchone():\n",
    "        _existing_dates={r[0].isoformat() if hasattr(r[0],'isoformat') else str(r[0]) for r in _con.execute(\"SELECT DISTINCT date FROM stock_quotes\").fetchall()}\n",
    "    _con.close()\n",
    "except Exception as e:\n",
    "    print(f\"Could not load existing quote dates: {e}\")\n",
    "print(f\"Existing quote days: {len(_existing_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2bd309f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-16 17:38:42,485 INFO EOD bulk 2025-09-16→2025-09-16: 1 new days (of 1 total), workers=32, symbols=11801\n",
      "2025-09-16 17:38:49,512 INFO Progress 1/1 days (fetched 0 non-empty frames)\n",
      "2025-09-16 17:38:49,516 INFO Combined rows (not counting already inserted days): 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "No new in-memory rows (all inserted directly or nothing new).\n"
     ]
    }
   ],
   "source": [
    "# Incremental EOD fetch using optimized function with skipping + streaming inserts\n",
    "import polars as pl\n",
    "allowed_symbols = set(stock_tickers.get_column(\"symbol\").to_list()) if 'stock_tickers' in globals() else None\n",
    "\n",
    "# Only fetch new days and stream insert directly to DuckDB (reduces memory and time for long histories)\n",
    "parallel_month_df = parallel_fetch_eod_bulk_range(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    allowed_symbols=allowed_symbols,\n",
    "    skip_existing=True,\n",
    "    existing_dates=_existing_dates,\n",
    "    insert_into_duckdb=True,  # Stream directly\n",
    ")\n",
    "\n",
    "# For quick inspection show just last few rows newly fetched (if any collected in-memory)\n",
    "print(parallel_month_df.shape)\n",
    "if not parallel_month_df.is_empty():\n",
    "    print(\"Unique Symbols (new batch):\", parallel_month_df.select(pl.col(\"symbol\").n_unique()).item())\n",
    "    print(parallel_month_df.tail())\n",
    "else:\n",
    "    print(\"No new in-memory rows (all inserted directly or nothing new).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91476ca0",
   "metadata": {},
   "source": [
    "### Load stock_quotes → DuckDB\n",
    "Create or replace americas.db.stock_quotes from the staged EOD bulk dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd0d8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new stock_quotes data to load.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2025, 9, 16, 0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb, polars as pl\n",
    "if 'parallel_month_df' in globals() and isinstance(parallel_month_df, pl.DataFrame) and not parallel_month_df.is_empty():\n",
    "    con = duckdb.connect('../americas.db')\n",
    "    con.register('new_quotes', parallel_month_df.to_pandas())\n",
    "\n",
    "    # Create table if missing\n",
    "    con.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS stock_quotes AS\n",
    "        SELECT * FROM new_quotes LIMIT 0\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert only rows whose (date,symbol) key not present\n",
    "    con.sql(\"\"\"\n",
    "        INSERT INTO stock_quotes\n",
    "        SELECT nq.*\n",
    "        FROM new_quotes nq\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1 FROM stock_quotes q\n",
    "            WHERE q.date = nq.date\n",
    "              AND q.symbol = nq.symbol\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    con.close()\n",
    "else:\n",
    "    print(\"No new stock_quotes data to load.\")\n",
    "\n",
    "con = db.connect('../americas.db')\n",
    "con.sql(\"SELECT MAX(date) AS max_date FROM stock_quotes\").fetchone()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf326c8c",
   "metadata": {},
   "source": [
    "## Metrics and Ratios (quarterly)\n",
    "\n",
    "Parallel fetch per‑symbol key metrics and ratios (last 100 periods), normalize/cast, join on (symbol, date, fiscalYear, period), and load to DuckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5450e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Ultra-optimized processing started in existing event loop...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting ULTRA-OPTIMIZED metrics processing...\n",
      "📊 Processing 11,801 symbols with advanced optimizations\n",
      "📅 Existing max date: 2025-07-05T00:00:00\n",
      "🎯 Threshold date: 2025-04-01\n",
      "🔄 Processing batch 1/6 (2000 symbols)\n",
      "✅ Batch completed: 0 successful frames in -12.3s (-0.0 frames/sec)\n",
      "🔄 Processing batch 2/6 (2000 symbols)\n",
      "✅ Batch completed: 0 successful frames in -13.8s (-0.0 frames/sec)\n",
      "🔄 Processing batch 3/6 (2000 symbols)\n",
      "✅ Batch completed: 0 successful frames in 47.1s (0.0 frames/sec)\n",
      "🔄 Processing batch 4/6 (2000 symbols)\n",
      "✅ Batch completed: 0 successful frames in 63.5s (0.0 frames/sec)\n",
      "🔄 Processing batch 5/6 (2000 symbols)\n",
      "✅ Batch completed: 0 successful frames in 64.5s (0.0 frames/sec)\n",
      "🔄 Processing batch 6/6 (1801 symbols)\n",
      "✅ Batch completed: 0 successful frames in 55.0s (0.0 frames/sec)\n",
      "❌ No data retrieved\n",
      "ℹ️ No new data to process\n"
     ]
    }
   ],
   "source": [
    "# ULTRA-OPTIMIZED Incremental fetch of Key Metrics + Ratios with AsyncIO + Advanced Optimizations\n",
    "# Performance improvements: Async I/O, Arrow integration, streaming, advanced caching, pipelining\n",
    "\n",
    "import os, time, asyncio, aiohttp, polars as pl, duckdb\n",
    "import concurrent.futures as cf\n",
    "from typing import Any, Dict, List, Optional, Set\n",
    "from datetime import date as _date, datetime\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import weakref\n",
    "\n",
    "API_KEY = os.getenv(\"FMP_API_KEY\")\n",
    "_common_params = {\"apikey\": API_KEY} if API_KEY else {}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Advanced Configuration & Caching\n",
    "# ------------------------------------------------------------------\n",
    "@dataclass\n",
    "class OptimizationConfig:\n",
    "    max_concurrent_requests: int = 100  # Higher concurrency for async\n",
    "    max_batch_size: int = 2000\n",
    "    cache_size: int = 10000\n",
    "    use_arrow_integration: bool = True\n",
    "    enable_streaming: bool = True\n",
    "    connection_timeout: int = 30\n",
    "    request_timeout: int = 45\n",
    "    retry_attempts: int = 2\n",
    "    backoff_factor: float = 1.2\n",
    "\n",
    "CONFIG = OptimizationConfig()\n",
    "\n",
    "# Simple LRU cache for API responses\n",
    "class SimpleCache:\n",
    "    def __init__(self, maxsize: int = 10000):\n",
    "        self.cache = {}\n",
    "        self.access_order = []\n",
    "        self.maxsize = maxsize\n",
    "    \n",
    "    def get(self, key: str) -> Optional[List[Dict]]:\n",
    "        if key in self.cache:\n",
    "            # Move to end (most recently used)\n",
    "            self.access_order.remove(key)\n",
    "            self.access_order.append(key)\n",
    "            return self.cache[key]\n",
    "        return None\n",
    "    \n",
    "    def put(self, key: str, value: List[Dict]):\n",
    "        if key in self.cache:\n",
    "            self.access_order.remove(key)\n",
    "        elif len(self.cache) >= self.maxsize:\n",
    "            # Remove least recently used\n",
    "            oldest = self.access_order.pop(0)\n",
    "            del self.cache[oldest]\n",
    "        \n",
    "        self.cache[key] = value\n",
    "        self.access_order.append(key)\n",
    "\n",
    "# Global cache instance\n",
    "API_CACHE = SimpleCache(CONFIG.cache_size)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Async HTTP Client with Advanced Features\n",
    "# ------------------------------------------------------------------\n",
    "class AsyncAPIClient:\n",
    "    def __init__(self):\n",
    "        self.session: Optional[aiohttp.ClientSession] = None\n",
    "        self.semaphore = asyncio.Semaphore(CONFIG.max_concurrent_requests)\n",
    "        \n",
    "    async def __aenter__(self):\n",
    "        connector = aiohttp.TCPConnector(\n",
    "            limit=CONFIG.max_concurrent_requests,\n",
    "            limit_per_host=50,\n",
    "            ttl_dns_cache=300,\n",
    "            use_dns_cache=True,\n",
    "            keepalive_timeout=60,\n",
    "            enable_cleanup_closed=True\n",
    "        )\n",
    "        \n",
    "        timeout = aiohttp.ClientTimeout(\n",
    "            total=CONFIG.connection_timeout + CONFIG.request_timeout,\n",
    "            connect=CONFIG.connection_timeout,\n",
    "            sock_read=CONFIG.request_timeout\n",
    "        )\n",
    "        \n",
    "        self.session = aiohttp.ClientSession(\n",
    "            connector=connector,\n",
    "            timeout=timeout,\n",
    "            headers={'User-Agent': 'FMP-Optimizer/2.0'}\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.session:\n",
    "            await self.session.close()\n",
    "    \n",
    "    async def fetch_json(self, url: str, params: Dict[str, Any]) -> List[Dict]:\n",
    "        \"\"\"Optimized async JSON fetcher with caching and smart retries\"\"\"\n",
    "        cache_key = f\"{url}?{sorted(params.items())}\"\n",
    "        \n",
    "        # Check cache first\n",
    "        cached_result = API_CACHE.get(cache_key)\n",
    "        if cached_result is not None:\n",
    "            return cached_result\n",
    "        \n",
    "        async with self.semaphore:  # Limit concurrent requests\n",
    "            for attempt in range(CONFIG.retry_attempts + 1):\n",
    "                try:\n",
    "                    async with self.session.get(url, params=params) as response:\n",
    "                        if response.status == 403:\n",
    "                            return []\n",
    "                        \n",
    "                        if response.status == 429 or response.status >= 500:\n",
    "                            if attempt < CONFIG.retry_attempts:\n",
    "                                await asyncio.sleep(CONFIG.backoff_factor ** attempt)\n",
    "                                continue\n",
    "                            return []\n",
    "                        \n",
    "                        response.raise_for_status()\n",
    "                        \n",
    "                        # Stream large JSON responses\n",
    "                        if CONFIG.enable_streaming:\n",
    "                            data = await response.json(loads=json.loads)\n",
    "                        else:\n",
    "                            text = await response.text()\n",
    "                            data = json.loads(text)\n",
    "                        \n",
    "                        # Handle various response formats\n",
    "                        if isinstance(data, dict):\n",
    "                            for v in data.values():\n",
    "                                if isinstance(v, list):\n",
    "                                    result = v\n",
    "                                    break\n",
    "                            else:\n",
    "                                result = []\n",
    "                        else:\n",
    "                            result = data if isinstance(data, list) else []\n",
    "                        \n",
    "                        # Cache successful results\n",
    "                        if result:\n",
    "                            API_CACHE.put(cache_key, result)\n",
    "                        \n",
    "                        return result\n",
    "                        \n",
    "                except asyncio.TimeoutError:\n",
    "                    if attempt < CONFIG.retry_attempts:\n",
    "                        await asyncio.sleep(CONFIG.backoff_factor ** attempt)\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    if attempt < CONFIG.retry_attempts:\n",
    "                        await asyncio.sleep(CONFIG.backoff_factor ** attempt)\n",
    "                        continue\n",
    "                    \n",
    "        return []\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Optimized Symbol Universe with Lazy Loading\n",
    "# ------------------------------------------------------------------\n",
    "def load_symbol_universe_optimized() -> List[str]:\n",
    "    \"\"\"Ultra-fast symbol loading with caching\"\"\"\n",
    "    cache_key = \"symbol_universe\"\n",
    "    cached = API_CACHE.get(cache_key)\n",
    "    if cached:\n",
    "        return [item['symbol'] for item in cached if 'symbol' in item]\n",
    "    \n",
    "    if 'stock_tickers' in globals() and isinstance(stock_tickers, pl.DataFrame):\n",
    "        # Use lazy scan with projection pushdown\n",
    "        symbols = (stock_tickers\n",
    "                  .lazy()\n",
    "                  .select(\"symbol\")\n",
    "                  .filter(pl.col(\"symbol\").is_not_null() & (pl.col(\"symbol\") != \"\"))\n",
    "                  .unique()\n",
    "                  .collect()\n",
    "                  .to_series()\n",
    "                  .to_list())\n",
    "        \n",
    "        # Cache the result\n",
    "        API_CACHE.put(cache_key, [{\"symbol\": s} for s in symbols])\n",
    "        return symbols\n",
    "    \n",
    "    try:\n",
    "        con = duckdb.connect('../americas.db')\n",
    "        # Use Arrow format for faster data transfer\n",
    "        result = con.execute(\"\"\"\n",
    "            SELECT DISTINCT symbol \n",
    "            FROM stock_tickers \n",
    "            WHERE symbol IS NOT NULL AND symbol != ''\n",
    "            ORDER BY symbol\n",
    "        \"\"\").arrow()\n",
    "        con.close()\n",
    "        \n",
    "        # Convert Arrow to list efficiently\n",
    "        symbols = result.column('symbol').to_pylist()\n",
    "        API_CACHE.put(cache_key, [{\"symbol\": s} for s in symbols])\n",
    "        return symbols\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Optimized Database Operations with Arrow Integration\n",
    "# ------------------------------------------------------------------\n",
    "def get_existing_metrics_info() -> tuple[bool, Optional[str]]:\n",
    "    \"\"\"Fast database introspection using Arrow format\"\"\"\n",
    "    try:\n",
    "        con = duckdb.connect('../americas.db')\n",
    "        \n",
    "        # Single optimized query using Arrow\n",
    "        result = con.execute(\"\"\"\n",
    "            WITH table_check AS (\n",
    "                SELECT COUNT(*) as table_exists\n",
    "                FROM information_schema.tables \n",
    "                WHERE table_name = 'stock_metrics'\n",
    "            ),\n",
    "            max_date AS (\n",
    "                SELECT CASE \n",
    "                    WHEN (SELECT table_exists FROM table_check) > 0 \n",
    "                    THEN (SELECT MAX(date) FROM stock_metrics)\n",
    "                    ELSE NULL \n",
    "                END as max_date\n",
    "            )\n",
    "            SELECT \n",
    "                (SELECT table_exists FROM table_check) > 0 as exists,\n",
    "                (SELECT max_date FROM max_date) as max_date\n",
    "        \"\"\").fetchone()\n",
    "        \n",
    "        con.close()\n",
    "        \n",
    "        table_exists = bool(result[0])\n",
    "        max_date = result[1].isoformat() if result[1] else None\n",
    "        \n",
    "        return table_exists, max_date\n",
    "        \n",
    "    except Exception:\n",
    "        return False, None\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Advanced Async Data Processing Pipeline\n",
    "# ------------------------------------------------------------------\n",
    "async def fetch_symbol_data_async(client: AsyncAPIClient, symbol: str, threshold_date: Optional[str]) -> Optional[pl.LazyFrame]:\n",
    "    \"\"\"Ultra-optimized async symbol data fetching with pipeline processing\"\"\"\n",
    "    \n",
    "    # Fetch both endpoints concurrently\n",
    "    metrics_task = client.fetch_json(\n",
    "        \"https://financialmodelingprep.com/stable/key-metrics\",\n",
    "        {\"symbol\": symbol, \"period\": \"quarter\", \"limit\": 100, **_common_params}\n",
    "    )\n",
    "    \n",
    "    ratios_task = client.fetch_json(\n",
    "        \"https://financialmodelingprep.com/stable/ratios\", \n",
    "        {\"symbol\": symbol, \"period\": \"quarter\", \"limit\": 100, **_common_params}\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        km_data, rt_data = await asyncio.gather(metrics_task, ratios_task, return_exceptions=True)\n",
    "        \n",
    "        # Handle exceptions\n",
    "        if isinstance(km_data, Exception):\n",
    "            km_data = []\n",
    "        if isinstance(rt_data, Exception):\n",
    "            rt_data = []\n",
    "            \n",
    "        if not km_data and not rt_data:\n",
    "            return None\n",
    "        \n",
    "        # Process data efficiently\n",
    "        def process_records(records: List[Dict], symbol: str) -> Optional[pl.LazyFrame]:\n",
    "            if not records:\n",
    "                return None\n",
    "                \n",
    "            # Optimize record processing\n",
    "            for r in records:\n",
    "                r['symbol'] = symbol  # Avoid .get() overhead\n",
    "                date_val = r.get('date') or r.get('reportedDate')\n",
    "                if date_val:\n",
    "                    r['date'] = date_val[:10]  # Fast string slice instead of split\n",
    "            \n",
    "            try:\n",
    "                df = pl.DataFrame(records)\n",
    "                lazy_df = df.lazy()\n",
    "                \n",
    "                # Apply threshold filter early\n",
    "                if threshold_date and 'date' in df.columns:\n",
    "                    lazy_df = lazy_df.filter(pl.col('date') >= threshold_date)\n",
    "                    \n",
    "                return lazy_df\n",
    "            except Exception:\n",
    "                return None\n",
    "        \n",
    "        km_lazy = process_records(km_data, symbol)\n",
    "        rt_lazy = process_records(rt_data, symbol)\n",
    "        \n",
    "        # Smart joining with schema optimization\n",
    "        if km_lazy is None and rt_lazy is None:\n",
    "            return None\n",
    "        elif km_lazy is None:\n",
    "            result = rt_lazy\n",
    "        elif rt_lazy is None:\n",
    "            result = km_lazy\n",
    "        else:\n",
    "            # Efficient join with column optimization\n",
    "            try:\n",
    "                km_schema = km_lazy.collect_schema()\n",
    "                rt_schema = rt_lazy.collect_schema()\n",
    "                \n",
    "                # Determine optimal join strategy\n",
    "                join_keys = [k for k in ('symbol', 'date', 'fiscalYear', 'period')\n",
    "                           if k in km_schema and k in rt_schema]\n",
    "                \n",
    "                if not join_keys:\n",
    "                    join_keys = ['symbol', 'date']\n",
    "                    join_keys = [k for k in join_keys if k in km_schema and k in rt_schema]\n",
    "                \n",
    "                if join_keys and len(join_keys) >= 2:\n",
    "                    # Use efficient join\n",
    "                    rt_only_cols = [c for c in rt_schema.names() if c not in km_schema.names()]\n",
    "                    if rt_only_cols:\n",
    "                        select_cols = join_keys + rt_only_cols\n",
    "                        result = km_lazy.join(rt_lazy.select(select_cols), on=join_keys, how='full')\n",
    "                    else:\n",
    "                        result = km_lazy\n",
    "                else:\n",
    "                    # Fallback to concat\n",
    "                    result = pl.concat([km_lazy, rt_lazy], how='vertical_relaxed')\n",
    "                    \n",
    "            except Exception:\n",
    "                # Ultimate fallback\n",
    "                result = pl.concat([km_lazy, rt_lazy], how='vertical_relaxed')\n",
    "        \n",
    "        # Optimize date parsing\n",
    "        if result and 'date' in result.collect_schema():\n",
    "            result = result.with_columns(\n",
    "                pl.col('date').str.strptime(pl.Date, \"%Y-%m-%d\", strict=False)\n",
    "            )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Main Optimized Processing Function\n",
    "# ------------------------------------------------------------------\n",
    "async def process_metrics_ultra_optimized():\n",
    "    \"\"\"Main ultra-optimized processing function with full async pipeline\"\"\"\n",
    "    \n",
    "    print(\"🚀 Starting ULTRA-OPTIMIZED metrics processing...\")\n",
    "    \n",
    "    # Load symbols with caching\n",
    "    symbols = load_symbol_universe_optimized()\n",
    "    print(f\"📊 Processing {len(symbols):,} symbols with advanced optimizations\")\n",
    "    \n",
    "    if not symbols:\n",
    "        print(\"❌ No symbols found\")\n",
    "        return pl.DataFrame()\n",
    "    \n",
    "    # Get existing data info efficiently\n",
    "    metrics_exists, existing_max_date = get_existing_metrics_info()\n",
    "    print(f\"📅 Existing max date: {existing_max_date}\")\n",
    "    \n",
    "    # Calculate threshold date\n",
    "    threshold_date = None\n",
    "    if existing_max_date:\n",
    "        try:\n",
    "            def _quarter_start(d: _date) -> _date:\n",
    "                return _date(d.year, ((d.month-1)//3)*3 + 1, 1)\n",
    "            \n",
    "            def _prev_quarter_start(d: _date) -> _date:\n",
    "                qs = _quarter_start(d)\n",
    "                m, y = qs.month - 3, qs.year\n",
    "                if m <= 0:\n",
    "                    m, y = m + 12, y - 1\n",
    "                return _date(y, m, 1)\n",
    "\n",
    "            emd = datetime.strptime(existing_max_date[:10], \"%Y-%m-%d\").date()\n",
    "            threshold_date = _prev_quarter_start(emd).isoformat()\n",
    "        except Exception:\n",
    "            threshold_date = existing_max_date\n",
    "    \n",
    "    print(f\"🎯 Threshold date: {threshold_date}\")\n",
    "    \n",
    "    # Process in optimized batches with async pipeline\n",
    "    batch_size = CONFIG.max_batch_size\n",
    "    all_lazy_frames = []\n",
    "    \n",
    "    async with AsyncAPIClient() as client:\n",
    "        for batch_start in range(0, len(symbols), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(symbols))\n",
    "            batch_symbols = symbols[batch_start:batch_end]\n",
    "            \n",
    "            print(f\"🔄 Processing batch {batch_start//batch_size + 1}/{(len(symbols) + batch_size - 1)//batch_size} ({len(batch_symbols)} symbols)\")\n",
    "            \n",
    "            # Create tasks for this batch\n",
    "            tasks = [\n",
    "                fetch_symbol_data_async(client, symbol, threshold_date)\n",
    "                for symbol in batch_symbols\n",
    "            ]\n",
    "            \n",
    "            # Process batch with progress tracking\n",
    "            start_time = time.time()\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # Filter successful results\n",
    "            batch_frames = []\n",
    "            for result in results:\n",
    "                if isinstance(result, pl.LazyFrame):\n",
    "                    batch_frames.append(result)\n",
    "                elif isinstance(result, Exception):\n",
    "                    continue  # Skip failed requests\n",
    "            \n",
    "            all_lazy_frames.extend(batch_frames)\n",
    "            \n",
    "            print(f\"✅ Batch completed: {len(batch_frames)} successful frames in {elapsed:.1f}s ({len(batch_frames)/elapsed:.1f} frames/sec)\")\n",
    "            \n",
    "            # Small delay to be nice to the API\n",
    "            if batch_end < len(symbols):\n",
    "                await asyncio.sleep(0.1)\n",
    "    \n",
    "    if not all_lazy_frames:\n",
    "        print(\"❌ No data retrieved\")\n",
    "        return pl.DataFrame()\n",
    "    \n",
    "    print(f\"🔗 Combining {len(all_lazy_frames)} lazy frames...\")\n",
    "    \n",
    "    try:\n",
    "        # Ultra-optimized combining with streaming\n",
    "        combined = pl.concat(all_lazy_frames, how='vertical_relaxed')\n",
    "        \n",
    "        # Apply final optimizations\n",
    "        combined = combined.unique(\n",
    "            subset=['symbol', 'date', 'fiscalYear', 'period'],\n",
    "            keep='last'\n",
    "        )\n",
    "        \n",
    "        # Apply threshold filter if needed\n",
    "        if threshold_date:\n",
    "            threshold_obj = datetime.strptime(threshold_date[:10], \"%Y-%m-%d\").date()\n",
    "            combined = combined.filter(pl.col('date') >= pl.lit(threshold_obj))\n",
    "        \n",
    "        # Collect final result\n",
    "        result = combined.collect()\n",
    "        \n",
    "        print(f\"🎉 Processing complete: {result.height:,} rows retrieved\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error combining results: {e}\")\n",
    "        return pl.DataFrame()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Ultra-Optimized DuckDB Integration\n",
    "# ------------------------------------------------------------------\n",
    "def save_to_duckdb_optimized(df: pl.DataFrame) -> bool:\n",
    "    \"\"\"Ultra-fast DuckDB insertion with Arrow integration\"\"\"\n",
    "    if df.is_empty():\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        con = duckdb.connect('../americas.db')\n",
    "        \n",
    "        # Use Arrow format for maximum speed\n",
    "        if CONFIG.use_arrow_integration:\n",
    "            arrow_table = df.to_arrow()\n",
    "            con.register('metrics_arrow', arrow_table)\n",
    "            source_table = 'metrics_arrow'\n",
    "        else:\n",
    "            con.register('metrics_new', df.to_pandas())\n",
    "            source_table = 'metrics_new'\n",
    "        \n",
    "        # Create table if needed\n",
    "        con.execute(f\"CREATE TABLE IF NOT EXISTS stock_metrics AS SELECT * FROM {source_table} LIMIT 0\")\n",
    "        \n",
    "        # Ultra-fast bulk upsert\n",
    "        key_cols = [c for c in ('symbol', 'date', 'fiscalYear', 'period') if c in df.columns]\n",
    "        if not key_cols:\n",
    "            key_cols = ['symbol', 'date']\n",
    "        \n",
    "        if key_cols:\n",
    "            key_conditions = ' AND '.join([f\"existing.{c} = new.{c}\" for c in key_cols])\n",
    "            \n",
    "            # Use efficient MERGE-like operation\n",
    "            insert_sql = f\"\"\"\n",
    "                INSERT INTO stock_metrics\n",
    "                SELECT new.* FROM {source_table} new\n",
    "                WHERE NOT EXISTS (\n",
    "                    SELECT 1 FROM stock_metrics existing \n",
    "                    WHERE {key_conditions}\n",
    "                )\n",
    "            \"\"\"\n",
    "            \n",
    "            rows_before = con.execute(\"SELECT COUNT(*) FROM stock_metrics\").fetchone()[0]\n",
    "            con.execute(insert_sql)\n",
    "            rows_after = con.execute(\"SELECT COUNT(*) FROM stock_metrics\").fetchone()[0]\n",
    "            \n",
    "            inserted = rows_after - rows_before\n",
    "            print(f\"💾 Inserted {inserted:,} new rows into stock_metrics\")\n",
    "        \n",
    "        con.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ DuckDB save failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Main Execution\n",
    "# ------------------------------------------------------------------\n",
    "async def main():\n",
    "    \"\"\"Ultra-optimized main execution function\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Process data with full optimizations\n",
    "        merged_df = await process_metrics_ultra_optimized()\n",
    "        \n",
    "        # Save to database if we have data\n",
    "        if not merged_df.is_empty():\n",
    "            success = save_to_duckdb_optimized(merged_df)\n",
    "            if success:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f\"🏁 ULTRA-OPTIMIZED processing completed in {elapsed:.1f}s\")\n",
    "                print(f\"📈 Performance: {len(merged_df)/elapsed:.1f} rows/second\")\n",
    "            else:\n",
    "                print(\"❌ Failed to save to database\")\n",
    "        else:\n",
    "            print(\"ℹ️ No new data to process\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ultra-optimization failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Execute the ultra-optimized version\n",
    "if __name__ == \"__main__\":\n",
    "    # For notebook execution\n",
    "    try:\n",
    "        # Check if we're in a notebook with existing event loop\n",
    "        loop = asyncio.get_running_loop()\n",
    "        # Create task in existing loop\n",
    "        task = loop.create_task(main())\n",
    "        # For notebook - you might need to await this differently\n",
    "        print(\"🔄 Ultra-optimized processing started in existing event loop...\")\n",
    "        merged_df = None  # Will be set by the async process\n",
    "    except RuntimeError:\n",
    "        # No existing event loop, create new one\n",
    "        merged_df = asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b526d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new key stock_metrics/ratios data to load.\n"
     ]
    }
   ],
   "source": [
    "# Load to DuckDB (incremental append on (symbol,date,fiscalYear,period))\n",
    "import polars as pl\n",
    "if 'merged_df' in globals() and isinstance(merged_df, pl.DataFrame) and not merged_df.is_empty():\n",
    "    import duckdb\n",
    "    con = duckdb.connect('../americas.db')\n",
    "    # Ensure base types for key columns\n",
    "    casts = []\n",
    "    if 'symbol' in merged_df.columns: casts.append(pl.col('symbol').cast(pl.Utf8, strict=False))\n",
    "    if 'date' in merged_df.columns: casts.append(pl.col('date').cast(pl.Date, strict=False))\n",
    "    if 'fiscalYear' in merged_df.columns: casts.append(pl.col('fiscalYear').cast(pl.Int64, strict=False))\n",
    "    if 'period' in merged_df.columns: casts.append(pl.col('period').cast(pl.Utf8, strict=False))\n",
    "    merged_df = merged_df.with_columns(casts) if casts else merged_df\n",
    "\n",
    "    con.register('metrics_new', merged_df.to_pandas())\n",
    "    # Create table if not exists\n",
    "    con.execute(\"CREATE TABLE IF NOT EXISTS stock_metrics AS SELECT * FROM metrics_new LIMIT 0\")\n",
    "\n",
    "    # Determine natural key columns present\n",
    "    key_cols = [c for c in ('symbol','date','fiscalYear','period') if c in merged_df.columns]\n",
    "    if not key_cols:  # fallback\n",
    "        key_cols = [c for c in ('symbol','date') if c in merged_df.columns]\n",
    "\n",
    "    # Build NOT EXISTS predicate\n",
    "    predicate = ' AND '.join([f\"m.{c} = n.{c}\" for c in key_cols]) if key_cols else '1=0'\n",
    "\n",
    "    insert_sql = f\"\"\"\n",
    "        INSERT INTO stock_metrics\n",
    "        SELECT n.* FROM metrics_new n\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1 FROM stock_metrics m WHERE {predicate}\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        con.execute(insert_sql)\n",
    "        new_rows = con.execute(\"SELECT COUNT(*) FROM metrics_new n WHERE NOT EXISTS (SELECT 1 FROM stock_metrics m WHERE \" + predicate + \")\").fetchone()[0] if False else None\n",
    "    except Exception as e:\n",
    "        print(f\"Incremental stock_metrics insert failed: {e}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "else:\n",
    "    print(\"No new key stock_metrics/ratios data to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8d973",
   "metadata": {},
   "source": [
    "## Exchanges → DuckDB\n",
    "\n",
    "Fetch all exchanges, keep only Americas via heuristic, and persist to americas.db.exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe17d8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 6)\n",
      "shape: (5, 6)\n",
      "┌──────────┬───────────────────────────┬──────────────────┬─────────────┬──────────────┬───────────┐\n",
      "│ exchange ┆ name                      ┆ country          ┆ countryCode ┆ symbolSuffix ┆ delay     │\n",
      "│ ---      ┆ ---                       ┆ ---              ┆ ---         ┆ ---          ┆ ---       │\n",
      "│ str      ┆ str                       ┆ str              ┆ str         ┆ str          ┆ str       │\n",
      "╞══════════╪═══════════════════════════╪══════════════════╪═════════════╪══════════════╪═══════════╡\n",
      "│ AMEX     ┆ New York Stock Exchange   ┆ United States of ┆ US          ┆ N/A          ┆ Real-time │\n",
      "│          ┆ Arca                      ┆ America          ┆             ┆              ┆           │\n",
      "│ BUE      ┆ Buenos Aires Stock        ┆ Argentina        ┆ AR          ┆ .BA          ┆ 20 min    │\n",
      "│          ┆ Exchange                  ┆                  ┆             ┆              ┆           │\n",
      "│ BVC      ┆ Colombia Stock Exchange   ┆ Colombia         ┆ CO          ┆ .CL          ┆ 15 min    │\n",
      "│ CBOE     ┆ Chicago Board Options     ┆ United States of ┆ US          ┆ N/A          ┆ Real-time │\n",
      "│          ┆ Exchange                  ┆ America          ┆             ┆              ┆           │\n",
      "│ CNQ      ┆ Canadian Securities       ┆ Canada           ┆ CA          ┆ .CN          ┆ Real-time │\n",
      "│          ┆ Exchange                  ┆                  ┆             ┆              ┆           │\n",
      "└──────────┴───────────────────────────┴──────────────────┴─────────────┴──────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Exchanges → DuckDB (incremental)\n",
    "import os, requests, polars as pl\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\"); _params={\"apikey\":API_KEY} if API_KEY else {}\n",
    "\n",
    "try:\n",
    "    r=requests.get(\"https://financialmodelingprep.com/stable/available-exchanges\", params=_params, timeout=60); r.raise_for_status(); ex_data=r.json(); ex_data=ex_data if isinstance(ex_data,list) else []\n",
    "except Exception: ex_data=[]\n",
    "\n",
    "CTRY={\"UNITED STATES\",\"CANADA\",\"BRAZIL\",\"MEXICO\",\"ARGENTINA\",\"CHILE\",\"PERU\",\"COLOMBIA\",\"VENEZUELA\",\"URUGUAY\",\"PARAGUAY\",\"BOLIVIA\",\"ECUADOR\",\"GUYANA\",\"SURINAME\",\"FRENCH GUIANA\",\"JAMAICA\",\"TRINIDAD AND TOBAGO\",\"TRINIDAD & TOBAGO\",\"BARBADOS\",\"BAHAMAS\",\"BERMUDA\",\"CAYMAN ISLANDS\",\"PANAMA\",\"COSTA RICA\",\"GUATEMALA\",\"HONDURAS\",\"EL SALVADOR\",\"NICARAGUA\",\"DOMINICAN REPUBLIC\",\"HAITI\",\"PUERTO RICO\",\"BELIZE\",\"CURACAO\",\"ARUBA\",\"SAINT LUCIA\",\"GRENADA\",\"ST. VINCENT AND THE GRENADINES\"}\n",
    "ISO2={\"US\",\"CA\",\"BR\",\"MX\",\"AR\",\"CL\",\"PE\",\"CO\",\"VE\",\"UY\",\"PY\",\"BO\",\"EC\",\"GY\",\"SR\",\"GF\",\"JM\",\"TT\",\"BB\",\"BS\",\"BM\",\"KY\",\"PA\",\"CR\",\"GT\",\"HN\",\"SV\",\"NI\",\"DO\",\"H\",\"PR\",\"BZ\",\"LC\",\"GD\",\"VC\",\"CW\",\"AW\"}\n",
    "\n",
    "def is_amer_exchange(rec: dict)->bool:\n",
    "    for k in (\"region\",\"continent\"):\n",
    "        v=rec.get(k)\n",
    "        if isinstance(v,str) and \"AMERICA\" in v.upper(): return True\n",
    "    for k in (\"countryName\",\"country\",\"countryCode\",\"country_code\",\"country_iso2\"):\n",
    "        v=rec.get(k); vu=v.strip().upper() if isinstance(v,str) else \"\"\n",
    "        if vu and ((len(vu)<=3 and vu in ISO2) or vu in CTRY or \"UNITED STATES\" in vu or \"LATIN AMERICA\" in vu): return True\n",
    "    return False\n",
    "\n",
    "ex_df = pl.DataFrame([rec for rec in ex_data if isinstance(rec,dict) and is_amer_exchange(rec)])\n",
    "if not ex_df.is_empty():\n",
    "    ren={}; ren[\"countryName\"]=\"country\" if \"countryName\" in ex_df.columns and \"country\" not in ex_df.columns else None; ren={k:v for k,v in ren.items() if v}\n",
    "    ex_df = ex_df.rename(ren) if ren else ex_df\n",
    "    ex_df = ex_df.with_columns([pl.all().cast(pl.Utf8, strict=False)])\n",
    "\n",
    "print(ex_df.shape); print(ex_df.head())\n",
    "\n",
    "if not ex_df.is_empty():\n",
    "    import duckdb\n",
    "    con=duckdb.connect('../americas.db')\n",
    "    con.register('exchanges_new', ex_df.to_pandas())\n",
    "    con.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS exchanges AS\n",
    "        SELECT * FROM exchanges_new LIMIT 0\n",
    "    \"\"\")\n",
    "    \n",
    "    # Check which columns exist in the dataframe\n",
    "    columns = ex_df.columns\n",
    "    \n",
    "    # Use exchange as the primary key since it's always present\n",
    "    if 'exchange' in columns:\n",
    "        con.sql(\"\"\"\n",
    "            INSERT INTO exchanges\n",
    "            SELECT e.* FROM exchanges_new e\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT 1 FROM exchanges x\n",
    "                WHERE x.exchange = e.exchange\n",
    "            )\n",
    "        \"\"\")\n",
    "    con.close()\n",
    "else:\n",
    "    print(\"No Americas exchanges found from API; skipping DuckDB load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879a578",
   "metadata": {},
   "source": [
    "## index_list → DuckDB\n",
    "\n",
    "Fetch index list, keep those on Americas exchanges, then persist as americas.db.index_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c2c23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 4)\n",
      "shape: (5, 4)\n",
      "┌────────┬─────────────────────────────────┬──────────┬──────────┐\n",
      "│ symbol ┆ name                            ┆ exchange ┆ currency │\n",
      "│ ---    ┆ ---                             ┆ ---      ┆ ---      │\n",
      "│ str    ┆ str                             ┆ str      ┆ str      │\n",
      "╞════════╪═════════════════════════════════╪══════════╪══════════╡\n",
      "│ ^TTIN  ┆ S&P/TSX Capped Industrials Ind… ┆ TSX      ┆ CAD      │\n",
      "│ ^NYA   ┆ NYSE Composite                  ┆ NYSE     ┆ USD      │\n",
      "│ ^XAX   ┆ NYSE American Composite Index   ┆ NYSE     ┆ USD      │\n",
      "│ ^NYITR ┆ NYSE International 100 Index    ┆ NYSE     ┆ USD      │\n",
      "│ ^DJU   ┆ Dow Jones Utility Average       ┆ NASDAQ   ┆ USD      │\n",
      "└────────┴─────────────────────────────────┴──────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# index_list → DuckDB (incremental)\n",
    "import os, requests, polars as pl\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\"); _params={\"apikey\":API_KEY} if API_KEY else {}\n",
    "\n",
    "try:\n",
    "    r=requests.get(\"https://financialmodelingprep.com/stable/index-list\", params=_params, timeout=60); r.raise_for_status(); idx_data=r.json(); idx_data=idx_data if isinstance(idx_data,list) else []\n",
    "except Exception: idx_data=[]\n",
    "\n",
    "amer_exchanges = {e.upper() for e in EXCHANGES_AMERICAS} if 'EXCHANGES_AMERICAS' in globals() and EXCHANGES_AMERICAS else {\"NYSE\",\"NASDAQ\",\"AMEX\",\"ARCX\",\"NYS\",\"NAS\",\"ARC\",\"BATS\",\"OTC\",\"TSX\",\"TSXV\",\"CSE\",\"XTSE\",\"XTSX\",\"CHIC\",\"B3\",\"BMFBOVESPA\",\"BOV\",\"XBOV\",\"XMEX\",\"BMV\"}\n",
    "idx_df = pl.DataFrame([rec for rec in idx_data if isinstance(rec,dict) and str(rec.get(\"exchange\",\"\" )).upper() in amer_exchanges])\n",
    "if not idx_df.is_empty(): idx_df=idx_df.with_columns([pl.all().cast(pl.Utf8, strict=False)])\n",
    "\n",
    "print(idx_df.shape); print(idx_df.head())\n",
    "\n",
    "if not idx_df.is_empty():\n",
    "    import duckdb\n",
    "    con=duckdb.connect('../americas.db')\n",
    "    con.register('index_list_new', idx_df.to_pandas())\n",
    "    con.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS index_list AS\n",
    "        SELECT * FROM index_list_new LIMIT 0\n",
    "    \"\"\")\n",
    "    # Natural key = symbol (assumed unique for index_list list)\n",
    "    con.sql(\"\"\"\n",
    "        INSERT INTO index_list\n",
    "        SELECT i.* FROM index_list_new i\n",
    "        WHERE NOT EXISTS (SELECT 1 FROM index_list x WHERE x.symbol = i.symbol)\n",
    "    \"\"\")\n",
    "    con.close()\n",
    "else:\n",
    "    print(\"No Americas index_list found from API; skipping DuckDB load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77159c2",
   "metadata": {},
   "source": [
    "## Index Quotes (historical light) → DuckDB\n",
    "\n",
    "Parallel fetch daily light stock_quotes for Americas index_list since 2010‑01‑01 and write to americas.db.index_quotes (unique by symbol,date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8103352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index symbols to fetch: 36\n",
      "(111524, 4)\n",
      "shape: (5, 4)\n",
      "┌─────────┬────────────┬────────────┬───────────┐\n",
      "│ symbol  ┆ date       ┆ price      ┆ volume    │\n",
      "│ ---     ┆ ---        ┆ ---        ┆ ---       │\n",
      "│ str     ┆ date       ┆ f64        ┆ i64       │\n",
      "╞═════════╪════════════╪════════════╪═══════════╡\n",
      "│ TX60.TS ┆ 2020-11-03 ┆ 948.200012 ┆ 94504821  │\n",
      "│ TX60.TS ┆ 2020-11-04 ┆ 952.299988 ┆ 179295563 │\n",
      "│ TX60.TS ┆ 2020-11-05 ┆ 968.52002  ┆ 127428692 │\n",
      "│ TX60.TS ┆ 2020-11-06 ┆ 966.869995 ┆ 118405024 │\n",
      "│ TX60.TS ┆ 2020-11-09 ┆ 980.429993 ┆ 246182781 │\n",
      "└─────────┴────────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Index stock_quotes (light) → DuckDB\n",
    "import os, time, requests, polars as pl, concurrent.futures as cf\n",
    "from io import StringIO\n",
    "\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\"); _params={\"apikey\": API_KEY} if API_KEY else {}\n",
    "BASE_URL=\"https://financialmodelingprep.com/stable/historical-price-eod/light\"\n",
    "\n",
    "# 1) Gather index symbols\n",
    "index_symbols = sorted({s for s in idx_df.get_column('symbol').to_list() if isinstance(s,str) and s.strip()}) if 'idx_df' in globals() and isinstance(idx_df, pl.DataFrame) and not idx_df.is_empty() else []\n",
    "if not index_symbols:\n",
    "    try:\n",
    "        import duckdb; con=duckdb.connect('../americas.db'); res=con.sql(\"SELECT symbol FROM index_list\").fetchall(); con.close(); index_symbols=sorted({r[0] for r in res if isinstance(r[0],str) and r[0].strip()})\n",
    "    except Exception: index_symbols=[]\n",
    "print(f\"Index symbols to fetch: {len(index_symbols)}\")\n",
    "\n",
    "# 2) Fetch per symbol\n",
    "def fetch_index_quotes(symbol: str, start: str=\"2010-01-01\", max_retries: int=3, sleep_base: float=0.4)->pl.DataFrame:\n",
    "    q={\"symbol\":symbol,\"from\":start, **_params}; a=0\n",
    "    while True:\n",
    "        try:\n",
    "            r=requests.get(BASE_URL, params=q, timeout=90); s=r.status_code\n",
    "            if s in (429,) or s>=500:\n",
    "                if a<max_retries: time.sleep(sleep_base*(2**a)); a+=1; continue\n",
    "            if s==403: return pl.DataFrame()\n",
    "            r.raise_for_status(); data=r.json(); data=[data] if isinstance(data,dict) else data\n",
    "            if not isinstance(data,list) or not data: return pl.DataFrame()\n",
    "            df=pl.DataFrame(data)\n",
    "            casts=[]\n",
    "            if \"date\" in df.columns: casts.append(pl.col(\"date\").cast(pl.Utf8).str.to_date(\"%Y-%m-%d\"))\n",
    "            if \"price\" in df.columns: casts.append(pl.col(\"price\").cast(pl.Float64, strict=False))\n",
    "            if \"volume\" in df.columns: casts.append(pl.col(\"volume\").cast(pl.Int64, strict=False))\n",
    "            df=df.with_columns(casts) if casts else df\n",
    "            if \"symbol\" not in df.columns: df=df.with_columns([pl.lit(symbol).alias(\"symbol\")])\n",
    "            return df\n",
    "        except Exception:\n",
    "            if a<max_retries: time.sleep(sleep_base*(2**a)); a+=1; continue\n",
    "            return pl.DataFrame()\n",
    "\n",
    "# 3) Parallel fetch\n",
    "frames=[]\n",
    "if index_symbols:\n",
    "    max_workers=min(16, max(4, (os.cpu_count() or 8)))\n",
    "    with cf.ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        for fut in cf.as_completed({ex.submit(fetch_index_quotes, s): s for s in index_symbols}):\n",
    "            df=fut.result(); frames.append(df) if isinstance(df,pl.DataFrame) and not df.is_empty() else None\n",
    "\n",
    "index_quotes_df=pl.DataFrame() if not frames else pl.concat(frames, how=\"vertical_relaxed\")\n",
    "\n",
    "# Unique by (symbol,date)\n",
    "if not index_quotes_df.is_empty():\n",
    "    keep=[c for c in index_quotes_df.columns if c in {\"symbol\",\"date\",\"price\",\"volume\"}] or index_quotes_df.columns\n",
    "    index_quotes_df=index_quotes_df.select(keep).unique(subset=[\"symbol\",\"date\"], keep=\"last\").sort([\"symbol\",\"date\"])\n",
    "\n",
    "print(index_quotes_df.shape); print(index_quotes_df.head())\n",
    "\n",
    "# 4) Load to DuckDB\n",
    "if not index_quotes_df.is_empty():\n",
    "    import duckdb\n",
    "    con=duckdb.connect('../americas.db'); con.sql(\"CREATE OR REPLACE TABLE index_quotes AS SELECT * FROM index_quotes_df\"); con.close()\n",
    "else:\n",
    "    print(\"No index stock_quotes fetched; skipping DuckDB load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6038489",
   "metadata": {},
   "source": [
    "## Market Risk Premium → DuckDB\n",
    "\n",
    "Fetch market risk premium data for all countries, filter to Americas region, and persist as americas.db.risk_premium.\n",
    "- API endpoint: https://financialmodelingprep.com/stable/market-risk-premium\n",
    "- Returns countryRiskPremium and totalEquityRiskPremium by country\n",
    "- Americas countries filtered using same logic as exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc6e603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 186 countries from Market Risk Premium API\n",
      "Filtered to 44 Americas countries\n",
      "Sample Americas risk premium data:\n",
      "  Venezuela: Country Risk=23.59%, Total Equity Risk=27.92%\n",
      "  Uruguay: Country Risk=2.13%, Total Equity Risk=6.46%\n",
      "  United States: Country Risk=0%, Total Equity Risk=4.33%\n",
      "  Turks & Caicos Islands: Country Risk=2.13%, Total Equity Risk=6.46%\n",
      "  Trinidad and Tobago: Country Risk=4.02%, Total Equity Risk=8.35%\n",
      "Risk premium DataFrame shape: (44, 4)\n",
      "shape: (5, 4)\n",
      "┌────────────────────────┬───────────────┬────────────────────┬────────────────────────┐\n",
      "│ country                ┆ continent     ┆ countryRiskPremium ┆ totalEquityRiskPremium │\n",
      "│ ---                    ┆ ---           ┆ ---                ┆ ---                    │\n",
      "│ str                    ┆ str           ┆ f64                ┆ f64                    │\n",
      "╞════════════════════════╪═══════════════╪════════════════════╪════════════════════════╡\n",
      "│ Venezuela              ┆ South America ┆ 23.59              ┆ 27.92                  │\n",
      "│ Uruguay                ┆ South America ┆ 2.13               ┆ 6.46                   │\n",
      "│ United States          ┆ North America ┆ 0.0                ┆ 4.33                   │\n",
      "│ Turks & Caicos Islands ┆ North America ┆ 2.13               ┆ 6.46                   │\n",
      "│ Trinidad and Tobago    ┆ North America ┆ 4.02               ┆ 8.35                   │\n",
      "└────────────────────────┴───────────────┴────────────────────┴────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Market Risk Premium → DuckDB (incremental)\n",
    "import os, requests, polars as pl\n",
    "API_KEY=os.getenv(\"FMP_API_KEY\"); _params={\"apikey\":API_KEY} if API_KEY else {}\n",
    "\n",
    "try:\n",
    "    r=requests.get(\"https://financialmodelingprep.com/stable/market-risk-premium\", params=_params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    risk_data=r.json()\n",
    "    risk_data=risk_data if isinstance(risk_data,list) else []\n",
    "    print(f\"Fetched {len(risk_data)} countries from Market Risk Premium API\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch market risk premium data: {e}\")\n",
    "    risk_data=[]\n",
    "\n",
    "# Define Americas countries (same logic as used in exchanges filter)\n",
    "CTRY_AMERICAS={\"UNITED STATES\",\"CANADA\",\"BRAZIL\",\"MEXICO\",\"ARGENTINA\",\"CHILE\",\"PERU\",\"COLOMBIA\",\"VENEZUELA\",\"URUGUAY\",\"PARAGUAY\",\"BOLIVIA\",\"ECUADOR\",\"GUYANA\",\"SURINAME\",\"FRENCH GUIANA\",\"JAMAICA\",\"TRINIDAD AND TOBAGO\",\"TRINIDAD & TOBAGO\",\"BARBADOS\",\"BAHAMAS\",\"BERMUDA\",\"CAYMAN ISLANDS\",\"PANAMA\",\"COSTA RICA\",\"GUATEMALA\",\"HONDURAS\",\"EL SALVADOR\",\"NICARAGUA\",\"DOMINICAN REPUBLIC\",\"HAITI\",\"PUERTO RICO\",\"BELIZE\",\"CURACAO\",\"ARUBA\",\"SAINT LUCIA\",\"GRENADA\",\"ST. VINCENT AND THE GRENADINES\"}\n",
    "ISO2_AMERICAS={\"US\",\"CA\",\"BR\",\"MX\",\"AR\",\"CL\",\"PE\",\"CO\",\"VE\",\"UY\",\"PY\",\"BO\",\"EC\",\"GY\",\"SR\",\"GF\",\"JM\",\"TT\",\"BB\",\"BS\",\"BM\",\"KY\",\"PA\",\"CR\",\"GT\",\"HN\",\"SV\",\"NI\",\"DO\",\"H\",\"PR\",\"BZ\",\"LC\",\"GD\",\"VC\",\"CW\",\"AW\"}\n",
    "\n",
    "def is_americas_country(rec: dict)->bool:\n",
    "    \"\"\"Check if country record is in Americas region\"\"\"\n",
    "    country = rec.get(\"country\", \"\").strip().upper()\n",
    "    continent = rec.get(\"continent\", \"\").strip().upper()\n",
    "    \n",
    "    # Check continent first\n",
    "    if continent and \"AMERICA\" in continent:\n",
    "        return True\n",
    "    \n",
    "    # Check country name\n",
    "    if country and (\n",
    "        country in CTRY_AMERICAS or\n",
    "        any(americas_country in country for americas_country in [\"UNITED STATES\", \"BRAZIL\", \"CANADA\", \"MEXICO\"]) or\n",
    "        (len(country) <= 3 and country in ISO2_AMERICAS)\n",
    "    ):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Filter to Americas countries only\n",
    "americas_risk_data = [rec for rec in risk_data if isinstance(rec, dict) and is_americas_country(rec)]\n",
    "print(f\"Filtered to {len(americas_risk_data)} Americas countries\")\n",
    "\n",
    "if americas_risk_data:\n",
    "    # Show sample of what we found\n",
    "    print(\"Sample Americas risk premium data:\")\n",
    "    for item in americas_risk_data[:5]:  # Show first 5\n",
    "        print(f\"  {item.get('country', 'N/A')}: Country Risk={item.get('countryRiskPremium', 'N/A')}%, Total Equity Risk={item.get('totalEquityRiskPremium', 'N/A')}%\")\n",
    "\n",
    "# Create DataFrame\n",
    "risk_premium_df = pl.DataFrame(americas_risk_data) if americas_risk_data else pl.DataFrame()\n",
    "\n",
    "if not risk_premium_df.is_empty():\n",
    "    # Ensure proper data types\n",
    "    casts = []\n",
    "    if \"countryRiskPremium\" in risk_premium_df.columns:\n",
    "        casts.append(pl.col(\"countryRiskPremium\").cast(pl.Float64, strict=False))\n",
    "    if \"totalEquityRiskPremium\" in risk_premium_df.columns:\n",
    "        casts.append(pl.col(\"totalEquityRiskPremium\").cast(pl.Float64, strict=False))\n",
    "    if \"country\" in risk_premium_df.columns:\n",
    "        casts.append(pl.col(\"country\").cast(pl.Utf8, strict=False))\n",
    "    if \"continent\" in risk_premium_df.columns:\n",
    "        casts.append(pl.col(\"continent\").cast(pl.Utf8, strict=False))\n",
    "    \n",
    "    risk_premium_df = risk_premium_df.with_columns(casts) if casts else risk_premium_df\n",
    "\n",
    "print(f\"Risk premium DataFrame shape: {risk_premium_df.shape}\")\n",
    "if not risk_premium_df.is_empty():\n",
    "    print(risk_premium_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98b191f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk premium table now contains 44 Americas countries\n",
      "\n",
      "Sample risk premium data (10 of total):\n",
      "  Anguilla (North America): Country Risk=8.11%, Total Equity Risk=12.44%\n",
      "  Antigua and Barbuda (North America): Country Risk=8.1%, Total Equity Risk=12.43%\n",
      "  Argentina (South America): Country Risk=16.02%, Total Equity Risk=20.35%\n",
      "  Aruba (North America): Country Risk=2.93%, Total Equity Risk=7.26%\n",
      "  Bahamas (North America): Country Risk=6.01%, Total Equity Risk=10.34%\n",
      "  Barbados (North America): Country Risk=8.68%, Total Equity Risk=13.01%\n",
      "  Belize (North America): Country Risk=10.01%, Total Equity Risk=14.34%\n",
      "  Bermuda (North America): Country Risk=1.13%, Total Equity Risk=5.46%\n",
      "  Bolivia (South America): Country Risk=13.35%, Total Equity Risk=17.68%\n",
      "  Brazil (South America): Country Risk=3.34%, Total Equity Risk=7.67%\n"
     ]
    }
   ],
   "source": [
    "# Load risk premium data to DuckDB (incremental by country)\n",
    "if 'risk_premium_df' in globals() and isinstance(risk_premium_df, pl.DataFrame) and not risk_premium_df.is_empty():\n",
    "    import duckdb\n",
    "    con = duckdb.connect('../americas.db')\n",
    "    \n",
    "    # Register the new data\n",
    "    con.register('risk_premium_new', risk_premium_df.to_pandas())\n",
    "    \n",
    "    # Create table if not exists\n",
    "    con.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS risk_premium AS\n",
    "        SELECT * FROM risk_premium_new LIMIT 0\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insert only new countries (unique by country name)\n",
    "    con.sql(\"\"\"\n",
    "        INSERT INTO risk_premium\n",
    "        SELECT rp.* FROM risk_premium_new rp\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1 FROM risk_premium x \n",
    "            WHERE UPPER(TRIM(x.country)) = UPPER(TRIM(rp.country))\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Check how many countries we have in total\n",
    "    total_countries = con.sql(\"SELECT COUNT(*) FROM risk_premium\").fetchone()[0]\n",
    "    new_countries = con.sql(\"\"\"\n",
    "        SELECT COUNT(*) FROM risk_premium_new rp\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1 FROM risk_premium x \n",
    "            WHERE UPPER(TRIM(x.country)) = UPPER(TRIM(rp.country))\n",
    "        )\n",
    "    \"\"\").fetchone()[0] if False else None  # Skip this count for performance\n",
    "    \n",
    "    print(f\"Risk premium table now contains {total_countries} Americas countries\")\n",
    "    con.close()\n",
    "else:\n",
    "    print(\"No risk premium data to load.\")\n",
    "\n",
    "# Show what's in the table\n",
    "try:\n",
    "    con = duckdb.connect('../americas.db')\n",
    "    if con.execute(\"SELECT 1 FROM information_schema.tables WHERE table_name='risk_premium'\").fetchone():\n",
    "        sample_data = con.sql(\"SELECT * FROM risk_premium ORDER BY country LIMIT 10\").fetchall()\n",
    "        print(f\"\\nSample risk premium data ({len(sample_data)} of total):\")\n",
    "        for row in sample_data:\n",
    "            print(f\"  {row[0]} ({row[1]}): Country Risk={row[2]}%, Total Equity Risk={row[3]}%\")\n",
    "    con.close()\n",
    "except Exception as e:\n",
    "    print(f\"Could not show sample data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e42204",
   "metadata": {},
   "source": [
    "## Verify database tables and record counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8794a44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "  - exchanges: 14 rows\n",
      "  - index_list: 36 rows\n",
      "  - index_quotes: 111,524 rows\n",
      "  - risk_premium: 44 rows\n",
      "  - stock_metrics: 525,189 rows\n",
      "  - stock_profiles: 11,859 rows\n",
      "  - stock_quotes: 34,094,058 rows\n",
      "  - stock_tickers: 11,859 rows\n",
      "Latest quote date: 2025-09-16 00:00:00\n",
      "Latest index quote date: 2025-09-16\n",
      "Risk premium countries: 44\n",
      "Sample risk premium data:\n",
      "  Anguilla: Country Risk=8.11%, Total Equity Risk=12.44%\n",
      "  Antigua and Barbuda: Country Risk=8.1%, Total Equity Risk=12.43%\n",
      "  Argentina: Country Risk=16.02%, Total Equity Risk=20.35%\n"
     ]
    }
   ],
   "source": [
    "# Verify database tables and record counts\n",
    "import duckdb\n",
    "con = duckdb.connect('../americas.db')\n",
    "tables = con.sql(\"SHOW TABLES\").fetchall()\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    count = con.sql(f\"SELECT COUNT(*) AS count FROM {table[0]}\").fetchone()[0]\n",
    "    print(f\"  - {table[0]}: {count:,} rows\")\n",
    "\n",
    "# Check the latest dates in the stock_quotes and index_quotes tables\n",
    "if any(t[0] == 'stock_quotes' for t in tables):\n",
    "    last_quote_date = con.sql(\"SELECT MAX(date) FROM stock_quotes\").fetchone()[0]\n",
    "    print(f\"Latest quote date: {last_quote_date}\")\n",
    "\n",
    "if any(t[0] == 'index_quotes' for t in tables):\n",
    "    last_index_date = con.sql(\"SELECT MAX(date) FROM index_quotes\").fetchone()[0] \n",
    "    print(f\"Latest index quote date: {last_index_date}\")\n",
    "\n",
    "# Show sample of risk premium data if table exists\n",
    "if any(t[0] == 'risk_premium' for t in tables):\n",
    "    risk_premium_count = con.sql(\"SELECT COUNT(*) FROM risk_premium\").fetchone()[0]\n",
    "    print(f\"Risk premium countries: {risk_premium_count}\")\n",
    "    if risk_premium_count > 0:\n",
    "        sample_risk = con.sql(\"SELECT country, countryRiskPremium, totalEquityRiskPremium FROM risk_premium ORDER BY country LIMIT 3\").fetchall()\n",
    "        print(\"Sample risk premium data:\")\n",
    "        for row in sample_risk:\n",
    "            print(f\"  {row[0]}: Country Risk={row[1]}%, Total Equity Risk={row[2]}%\")\n",
    "\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
